START ti 7.2.2023 05.04.33 +0200
epochs: 4, learning rate: 2e-5, batch size: 8, prediction treshold: 0.6, model: TurkuNLP/bert-base-finnish-cased-v1 
opus-mt translated train and test
Namespace(train=['data/train-opus-mt-translated.csv'], test='data/test-opus-mt-translated3.csv', model='TurkuNLP/bert-base-finnish-cased-v1', batch=8, epochs=4, learning=2e-05, threshold=None, loss=True, dev=True, clean_as_label=True, binary=False, save='opus-mt-finbert-base')
id                       object
label_identity_attack     int64
label_insult              int64
label_obscene             int64
label_severe_toxicity     int64
label_threat              int64
label_toxicity            int64
lang                     object
text                     object
dtype: object
                      id  ...                                               text
0       ee9697785fe41ff8  ...                            " Kiitos Xeno. - Puhu -
1       29fec512f2ee929e  ...             2009 (UTC) Kiinteä 03:36, 8. kesäkuuta
2       88944b29dde50648  ...               Kysymys Mitä vikaa korjauksessa oli?
3       c7bf1f59096102f3  ...  Olen samaa mieltä nyt, itse asiassa. (Hämmästy...
4       7d71ee0e8ea0794a  ...  Kisumu Näin, että edistitte Kisumua, mietin, v...
...                  ...  ...                                                ...
159566  5dd74c5c9e45c9a5  ...  " Sen sijaan, että tuhlaisit aikaa ad hominems...
159567  de28d8aa910d3463  ...  En yritä voittaa montaa ystävää, vaan yksinker...
159568  63dd6b07c99675b7  ...  17. syyskuuta. Edellinen allekirjoittamaton ko...
159569  1cf9756715ee09de  ...  Yritän olla varovainen ytmndin kanssa. on hyvä...
159570  fb6977954cc68910  ...  Vaikka uskon, että kaikki olisivat voineet hoi...

[159571 rows x 9 columns]
text      object
labels    object
dtype: object
id                       object
label_identity_attack     int64
label_insult              int64
label_obscene             int64
label_severe_toxicity     int64
label_threat              int64
label_toxicity            int64
lang                     object
text                     object
dtype: object
                     id  ...                                               text
0      879ad7bdba4cedaa  ...  " Hei Pieter pietersen, ja Tervetuloa Wikipedi...
1      8889526d5dccab4a  ...  " Sinut on väliaikaisesti estetty muokkaamasta...
2      3f49e23388bc4c07  ...  unblock Tule!!! Fuck..... okei mies, olen tode...
3      2bf685b152948de4  ...  " Zeqin kieltäminen Zeqin välitysjutun korjaus...
4      02511a5f1990bec2  ...          . Tämä tili on Dantherockerin sukkanukke1
...                 ...  ...                                                ...
63973  055d985a27c35d8d  ...  ". No, se ja laiska ylimielisyys. Kuinka monta...
63974  882dc2e32fd1e881  ...  Tämä menetelmä on surkea, jacobian on päivityk...
63975  874003b0dbc178cb  ...  " Valituksia toisesta päätoimittajasta  Kehota...
63976  9a0b5e24d59e8298  ...  WP:Päivän kuva  Minä loin sen kategorian, Muis...
63977  0d33b8948c212a88  ...      muista huumeista jälkeenjääneistä henkilöistä

[63978 rows x 9 columns]
text      object
labels    object
dtype: object
tensor([16.2248,  2.8940,  2.6981, 14.2921, 47.6901,  1.4905,  0.1590],
       device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 127656
    })
    dev: Dataset({
        features: ['text', 'labels'],
        num_rows: 31915
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2176, 'learning_rate': 1.9216644732719185e-05, 'epoch': 0.16}
Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.49      0.21      0.29       281
         label_insult       0.55      0.86      0.67      1527
        label_obscene       0.57      0.84      0.68      1647
label_severe_toxicity       0.40      0.50      0.45       328
         label_threat       0.00      0.00      0.00       100
       label_toxicity       0.68      0.83      0.75      2973

            micro avg       0.60      0.78      0.68      6856
            macro avg       0.45      0.54      0.47      6856
         weighted avg       0.59      0.78      0.67      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.2181689590215683, 'eval_f1': 0.6801491499715603, 'eval_f1_macro': 0.4720119403987777, 'eval_precision': 0.6000892160142746, 'eval_recall': 0.7848599766627772, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8827188842010552, 'eval_macro_roc_auc': 0.7593644619155211, 'eval_accuracy': 0.8969450101832994, 'eval_hamming loss': 0.02642957856807144, 'eval_runtime': 162.1009, 'eval_samples_per_second': 196.884, 'eval_steps_per_second': 12.307, 'epoch': 0.16}
{'loss': 0.1948, 'learning_rate': 1.8433289465438365e-05, 'epoch': 0.31}
Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.37      0.33      0.35       281
         label_insult       0.54      0.86      0.67      1527
        label_obscene       0.64      0.83      0.72      1647
label_severe_toxicity       0.46      0.32      0.38       328
         label_threat       0.54      0.38      0.45       100
       label_toxicity       0.70      0.83      0.76      2973

            micro avg       0.62      0.79      0.70      6856
            macro avg       0.54      0.59      0.55      6856
         weighted avg       0.62      0.79      0.69      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.20385409891605377, 'eval_f1': 0.6954782945236406, 'eval_f1_macro': 0.5539981586298833, 'eval_precision': 0.6234532207702094, 'eval_recall': 0.786318553092182, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8843418323050519, 'eval_macro_roc_auc': 0.7871351638298835, 'eval_accuracy': 0.9010496631677894, 'eval_hamming loss': 0.024654028931014673, 'eval_runtime': 162.1575, 'eval_samples_per_second': 196.815, 'eval_steps_per_second': 12.303, 'epoch': 0.31}
{'loss': 0.1929, 'learning_rate': 1.764993419815755e-05, 'epoch': 0.47}
Best threshold: 0.44999999999999996
                       precision    recall  f1-score   support

label_identity_attack       0.35      0.64      0.45       281
         label_insult       0.57      0.86      0.69      1527
        label_obscene       0.66      0.82      0.73      1647
label_severe_toxicity       0.43      0.37      0.40       328
         label_threat       0.44      0.52      0.48       100
       label_toxicity       0.69      0.84      0.76      2973

            micro avg       0.62      0.80      0.70      6856
            macro avg       0.52      0.67      0.58      6856
         weighted avg       0.63      0.80      0.70      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.14775043725967407, 'eval_f1': 0.7002544529262086, 'eval_f1_macro': 0.5834659704929158, 'eval_precision': 0.6209386281588448, 'eval_recall': 0.8028004667444574, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8923011508630483, 'eval_macro_roc_auc': 0.8276161615176757, 'eval_accuracy': 0.8991070029766568, 'eval_hamming loss': 0.02460702908768082, 'eval_runtime': 161.7563, 'eval_samples_per_second': 197.303, 'eval_steps_per_second': 12.333, 'epoch': 0.47}
{'loss': 0.177, 'learning_rate': 1.6866578930876732e-05, 'epoch': 0.63}
Best threshold: 0.39999999999999997
                       precision    recall  f1-score   support

label_identity_attack       0.28      0.66      0.40       281
         label_insult       0.59      0.87      0.70      1527
        label_obscene       0.68      0.83      0.75      1647
label_severe_toxicity       0.39      0.65      0.49       328
         label_threat       0.49      0.37      0.42       100
       label_toxicity       0.81      0.77      0.79      2973

            micro avg       0.65      0.79      0.71      6856
            macro avg       0.54      0.69      0.59      6856
         weighted avg       0.68      0.79      0.72      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.15955200791358948, 'eval_f1': 0.7125271542360608, 'eval_f1_macro': 0.5909849399762922, 'eval_precision': 0.6493101379724056, 'eval_recall': 0.7893815635939323, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8867751216260334, 'eval_macro_roc_auc': 0.8371545361099398, 'eval_accuracy': 0.9107942973523422, 'eval_hamming loss': 0.02280536842654969, 'eval_runtime': 162.1687, 'eval_samples_per_second': 196.801, 'eval_steps_per_second': 12.302, 'epoch': 0.63}
{'loss': 0.1926, 'learning_rate': 1.6083223663595916e-05, 'epoch': 0.78}
Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.49      0.47      0.48       281
         label_insult       0.65      0.83      0.73      1527
        label_obscene       0.70      0.82      0.75      1647
label_severe_toxicity       0.46      0.47      0.46       328
         label_threat       0.46      0.63      0.53       100
       label_toxicity       0.79      0.80      0.79      2973

            micro avg       0.70      0.78      0.74      6856
            macro avg       0.59      0.67      0.62      6856
         weighted avg       0.70      0.78      0.74      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.16871003806591034, 'eval_f1': 0.736682097699164, 'eval_f1_macro': 0.6248831856635435, 'eval_precision': 0.6998818432453722, 'eval_recall': 0.7775670945157527, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.882592921479309, 'eval_macro_roc_auc': 0.828028713037713, 'eval_accuracy': 0.9150242832523892, 'eval_hamming loss': 0.01990182254948039, 'eval_runtime': 162.0658, 'eval_samples_per_second': 196.926, 'eval_steps_per_second': 12.31, 'epoch': 0.78}
{'loss': 0.1677, 'learning_rate': 1.52998683963151e-05, 'epoch': 0.94}
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.61      0.42      0.50       281
         label_insult       0.62      0.85      0.72      1527
        label_obscene       0.62      0.87      0.73      1647
label_severe_toxicity       0.38      0.66      0.48       328
         label_threat       0.54      0.45      0.49       100
       label_toxicity       0.80      0.77      0.79      2973

            micro avg       0.67      0.79      0.72      6856
            macro avg       0.60      0.67      0.62      6856
         weighted avg       0.69      0.79      0.73      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.1448926031589508, 'eval_f1': 0.7225011715873335, 'eval_f1_macro': 0.6166679371807685, 'eval_precision': 0.6677391411953966, 'eval_recall': 0.7870478413068844, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8862527788269098, 'eval_macro_roc_auc': 0.8274955445900117, 'eval_accuracy': 0.9122042926523578, 'eval_hamming loss': 0.02164603895764792, 'eval_runtime': 161.9244, 'eval_samples_per_second': 197.098, 'eval_steps_per_second': 12.321, 'epoch': 0.94}
{'loss': 0.148, 'learning_rate': 1.451651312903428e-05, 'epoch': 1.1}
Best threshold: 0.44999999999999996
                       precision    recall  f1-score   support

label_identity_attack       0.45      0.57      0.51       281
         label_insult       0.65      0.82      0.73      1527
        label_obscene       0.70      0.84      0.76      1647
label_severe_toxicity       0.43      0.54      0.48       328
         label_threat       0.48      0.56      0.52       100
       label_toxicity       0.76      0.82      0.79      2973

            micro avg       0.68      0.80      0.74      6856
            macro avg       0.58      0.69      0.63      6856
         weighted avg       0.69      0.80      0.74      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.147248312830925, 'eval_f1': 0.7362889098504244, 'eval_f1_macro': 0.6294945310615921, 'eval_precision': 0.6841973453543702, 'eval_recall': 0.7969661610268378, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8916533525109924, 'eval_macro_roc_auc': 0.8385244268591735, 'eval_accuracy': 0.9125802913990286, 'eval_hamming loss': 0.020439709645412293, 'eval_runtime': 161.9055, 'eval_samples_per_second': 197.121, 'eval_steps_per_second': 12.322, 'epoch': 1.1}
{'loss': 0.1288, 'learning_rate': 1.3733157861753463e-05, 'epoch': 1.25}
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.46      0.49      0.47       281
         label_insult       0.70      0.81      0.75      1527
        label_obscene       0.76      0.79      0.78      1647
label_severe_toxicity       0.35      0.67      0.46       328
         label_threat       0.42      0.72      0.53       100
       label_toxicity       0.82      0.77      0.80      2973

            micro avg       0.71      0.77      0.74      6856
            macro avg       0.59      0.71      0.63      6856
         weighted avg       0.74      0.77      0.75      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.14390572905540466, 'eval_f1': 0.7402250351617441, 'eval_f1_macro': 0.6306833230210481, 'eval_precision': 0.7146931015752308, 'eval_recall': 0.7676487747957993, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8781347527693913, 'eval_macro_roc_auc': 0.8480853216837004, 'eval_accuracy': 0.9158076139746201, 'eval_hamming loss': 0.01929082458614027, 'eval_runtime': 161.7831, 'eval_samples_per_second': 197.27, 'eval_steps_per_second': 12.331, 'epoch': 1.25}
{'loss': 0.138, 'learning_rate': 1.2949802594472647e-05, 'epoch': 1.41}
Best threshold: 0.5499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.46      0.53      0.49       281
         label_insult       0.69      0.80      0.74      1527
        label_obscene       0.71      0.84      0.77      1647
label_severe_toxicity       0.44      0.48      0.46       328
         label_threat       0.48      0.49      0.49       100
       label_toxicity       0.81      0.80      0.80      2973

            micro avg       0.72      0.78      0.75      6856
            macro avg       0.60      0.66      0.63      6856
         weighted avg       0.72      0.78      0.75      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.16445040702819824, 'eval_f1': 0.7477748966290559, 'eval_f1_macro': 0.6263018142876055, 'eval_precision': 0.719681640361527, 'eval_recall': 0.7781505250875146, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8834479133014725, 'eval_macro_roc_auc': 0.8229264072247343, 'eval_accuracy': 0.9186902710324298, 'eval_hamming loss': 0.018794715128727348, 'eval_runtime': 161.666, 'eval_samples_per_second': 197.413, 'eval_steps_per_second': 12.34, 'epoch': 1.41}
{'loss': 0.1435, 'learning_rate': 1.2166447327191829e-05, 'epoch': 1.57}
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.51      0.56      0.53       281
         label_insult       0.69      0.82      0.75      1527
        label_obscene       0.74      0.82      0.78      1647
label_severe_toxicity       0.47      0.47      0.47       328
         label_threat       0.46      0.64      0.54       100
       label_toxicity       0.77      0.83      0.80      2973

            micro avg       0.71      0.80      0.75      6856
            macro avg       0.61      0.69      0.64      6856
         weighted avg       0.72      0.80      0.75      6856
          samples avg       0.07      0.07      0.07      6856

{'eval_loss': 0.13606928288936615, 'eval_f1': 0.7529850231209883, 'eval_f1_macro': 0.6447694343143693, 'eval_precision': 0.7146600288222194, 'eval_recall': 0.7956534422403734, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.891928565850843, 'eval_macro_roc_auc': 0.8390764444289299, 'eval_accuracy': 0.9160269465768447, 'eval_hamming loss': 0.018690271032429893, 'eval_runtime': 161.8653, 'eval_samples_per_second': 197.17, 'eval_steps_per_second': 12.325, 'epoch': 1.57}
{'loss': 0.1367, 'learning_rate': 1.1383092059911012e-05, 'epoch': 1.72}
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.41      0.63      0.49       281
         label_insult       0.60      0.89      0.71      1527
        label_obscene       0.66      0.87      0.75      1647
label_severe_toxicity       0.36      0.75      0.49       328
         label_threat       0.46      0.57      0.51       100
       label_toxicity       0.77      0.82      0.80      2973

            micro avg       0.65      0.83      0.73      6856
            macro avg       0.54      0.76      0.63      6856
         weighted avg       0.67      0.83      0.74      6856
          samples avg       0.06      0.08      0.06      6856

{'eval_loss': 0.1368028074502945, 'eval_f1': 0.7280947529291901, 'eval_f1_macro': 0.625722450343466, 'eval_precision': 0.646134719710669, 'eval_recall': 0.8338681446907817, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.9084551356381756, 'eval_macro_roc_auc': 0.8689711876700609, 'eval_accuracy': 0.9098543004856651, 'eval_hamming loss': 0.022298814559507024, 'eval_runtime': 161.7558, 'eval_samples_per_second': 197.304, 'eval_steps_per_second': 12.333, 'epoch': 1.72}
{'loss': 0.1385, 'learning_rate': 1.0599736792630196e-05, 'epoch': 1.88}
Best threshold: 0.5499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.55      0.48      0.51       281
         label_insult       0.68      0.83      0.75      1527
        label_obscene       0.71      0.85      0.77      1647
label_severe_toxicity       0.44      0.58      0.50       328
         label_threat       0.47      0.59      0.52       100
       label_toxicity       0.81      0.80      0.80      2973

            micro avg       0.72      0.79      0.75      6856
            macro avg       0.61      0.69      0.64      6856
         weighted avg       0.72      0.79      0.75      6856
          samples avg       0.06      0.07      0.06      6856

{'eval_loss': 0.157944455742836, 'eval_f1': 0.751143768196312, 'eval_f1_macro': 0.6421107583425972, 'eval_precision': 0.7157199471598414, 'eval_recall': 0.7902567094515752, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8893006090234792, 'eval_macro_roc_auc': 0.837370251488793, 'eval_accuracy': 0.9182829390568699, 'eval_hamming loss': 0.01874771528539349, 'eval_runtime': 161.857, 'eval_samples_per_second': 197.18, 'eval_steps_per_second': 12.326, 'epoch': 1.88}
{'loss': 0.1167, 'learning_rate': 9.816381525349378e-06, 'epoch': 2.04}
Best threshold: 0.5499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.45      0.57      0.50       281
         label_insult       0.70      0.80      0.75      1527
        label_obscene       0.69      0.86      0.77      1647
label_severe_toxicity       0.47      0.44      0.45       328
         label_threat       0.49      0.59      0.54       100
       label_toxicity       0.78      0.83      0.80      2973

            micro avg       0.70      0.80      0.75      6856
            macro avg       0.60      0.68      0.63      6856
         weighted avg       0.71      0.80      0.75      6856
          samples avg       0.06      0.07      0.07      6856

{'eval_loss': 0.15698862075805664, 'eval_f1': 0.7475688261881933, 'eval_f1_macro': 0.6341979335841242, 'eval_precision': 0.7046217402530338, 'eval_recall': 0.7960910151691949, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.891849465685489, 'eval_macro_roc_auc': 0.8337166009972976, 'eval_accuracy': 0.916058279805734, 'eval_hamming loss': 0.019249046947621286, 'eval_runtime': 161.798, 'eval_samples_per_second': 197.252, 'eval_steps_per_second': 12.33, 'epoch': 2.04}
{'loss': 0.0947, 'learning_rate': 9.03302625806856e-06, 'epoch': 2.19}
Best threshold: 0.44999999999999996
                       precision    recall  f1-score   support

label_identity_attack       0.47      0.50      0.48       281
         label_insult       0.72      0.79      0.75      1527
        label_obscene       0.74      0.82      0.78      1647
label_severe_toxicity       0.50      0.41      0.45       328
         label_threat       0.55      0.61      0.58       100
       label_toxicity       0.76      0.83      0.79      2973

            micro avg       0.72      0.78      0.75      6856
            macro avg       0.62      0.66      0.64      6856
         weighted avg       0.72      0.78      0.75      6856
          samples avg       0.07      0.07      0.07      6856

{'eval_loss': 0.18245773017406464, 'eval_f1': 0.7505246956765077, 'eval_f1_macro': 0.6397662186942766, 'eval_precision': 0.7211616025813391, 'eval_recall': 0.7823803967327888, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8855736813651921, 'eval_macro_roc_auc': 0.8244190020472159, 'eval_accuracy': 0.9163089456368478, 'eval_hamming loss': 0.018622382369836547, 'eval_runtime': 161.8293, 'eval_samples_per_second': 197.214, 'eval_steps_per_second': 12.328, 'epoch': 2.19}
{'loss': 0.0965, 'learning_rate': 8.249670990787743e-06, 'epoch': 2.35}
Best threshold: 0.39999999999999997
                       precision    recall  f1-score   support

label_identity_attack       0.36      0.62      0.45       281
         label_insult       0.72      0.79      0.75      1527
        label_obscene       0.80      0.79      0.79      1647
label_severe_toxicity       0.42      0.48      0.45       328
         label_threat       0.51      0.60      0.55       100
       label_toxicity       0.78      0.82      0.80      2973

            micro avg       0.72      0.78      0.75      6856
            macro avg       0.60      0.68      0.63      6856
         weighted avg       0.73      0.78      0.75      6856
          samples avg       0.07      0.07      0.07      6856

{'eval_loss': 0.17426511645317078, 'eval_f1': 0.7479015109121433, 'eval_f1_macro': 0.6334464784624556, 'eval_precision': 0.7185483870967742, 'eval_recall': 0.7797549591598599, 'eval_probs_roc_auc': 1.0, 'eval_micro_roc_auc': 0.8842068013733159, 'eval_macro_roc_auc': 0.8363848755515697, 'eval_accuracy': 0.9169982766724111, 'eval_hamming loss': 0.018820826152801715, 'eval_runtime': 161.738, 'eval_samples_per_second': 197.325, 'eval_steps_per_second': 12.335, 'epoch': 2.35}
{'train_runtime': 6659.7314, 'train_samples_per_second': 76.673, 'train_steps_per_second': 9.584, 'train_loss': 0.15226354370117187, 'epoch': 2.35}
saved
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.55      0.62      0.58       712
         label_insult       0.55      0.76      0.64      3427
        label_obscene       0.54      0.77      0.64      3691
label_severe_toxicity       0.25      0.48      0.33       367
         label_threat       0.43      0.64      0.51       211
       label_toxicity       0.50      0.87      0.63      6090

            micro avg       0.51      0.79      0.62     14498
            macro avg       0.47      0.69      0.56     14498
         weighted avg       0.52      0.79      0.62     14498
          samples avg       0.07      0.08      0.07     14498

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'epoch': 2.35,
 'eval_accuracy': 0.856825783863203,
 'eval_f1': 0.6234675056959966,
 'eval_f1_macro': 0.5569007477689395,
 'eval_hamming loss': 0.03616347286046245,
 'eval_loss': 0.1788463294506073,
 'eval_macro_roc_auc': 0.8299529182091753,
 'eval_micro_roc_auc': 0.8816412970998745,
 'eval_precision': 0.5137684398748323,
 'eval_probs_roc_auc': 1.0,
 'eval_recall': 0.7927300317285143,
 'eval_runtime': 405.2156,
 'eval_samples_per_second': 157.886,
 'eval_steps_per_second': 9.869}
F1: 0.6234675056959966
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.55      0.62      0.58       712
         label_insult       0.55      0.76      0.64      3427
        label_obscene       0.54      0.77      0.64      3691
label_severe_toxicity       0.25      0.48      0.33       367
         label_threat       0.43      0.64      0.51       211
       label_toxicity       0.50      0.87      0.63      6090

            micro avg       0.51      0.79      0.62     14498
            macro avg       0.47      0.69      0.56     14498
         weighted avg       0.52      0.79      0.62     14498
          samples avg       0.07      0.08      0.07     14498

Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.33      0.77      0.46       712
         label_insult       0.44      0.86      0.58      3427
        label_obscene       0.44      0.87      0.59      3691
label_severe_toxicity       0.20      0.76      0.31       367
         label_threat       0.37      0.73      0.49       211
       label_toxicity       0.46      0.90      0.61      6090

            micro avg       0.43      0.87      0.58     14498
            macro avg       0.37      0.81      0.51     14498
         weighted avg       0.44      0.87      0.58     14498
          samples avg       0.07      0.08      0.07     14498

{0: 'label_identity_attack', 1: 'label_insult', 2: 'label_obscene', 3: 'label_severe_toxicity', 4: 'label_threat', 5: 'label_toxicity'}
63978 63978 63978
END: ti 7.2.2023 07.11.13 +0200
