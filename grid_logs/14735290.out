START 14735290: Thu Dec 22 17:07:33 EET 2022
Namespace(train=['data/train_en_backtr_deepl.jsonl'], test='data/test_en_backtr_deepl.jsonl', model='bert-large-cased', batch=8, epochs=10, learning=2e-05, threshold=None, loss=True, dev=True, clean_as_label=True, binary=False, save=None)
['data/train_en_backtr_deepl.jsonl']
text      object
labels    object
dtype: object
text      object
labels    object
dtype: object
tensor([16.2248,  2.8940,  2.6981, 14.2921, 47.6901,  1.4905,  0.1590],
       device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 127656
    })
    dev: Dataset({
        features: ['text', 'labels'],
        num_rows: 31915
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2246, 'learning_rate': 1.9686657893087675e-05, 'epoch': 0.16}
Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.19      0.34      0.24       280
         label_insult       0.51      0.89      0.65      1618
        label_obscene       0.73      0.78      0.75      1705
label_severe_toxicity       0.37      0.60      0.46       318
         label_threat       0.00      0.00      0.00       114
       label_toxicity       0.78      0.76      0.77      3078

            micro avg       0.62      0.76      0.69      7113
            macro avg       0.43      0.56      0.48      7113
         weighted avg       0.65      0.76      0.69      7113
          samples avg       0.06      0.07      0.06      7113

{'eval_loss': 0.2438363879919052, 'eval_f1': 0.685424588086185, 'eval_f1_macro': 0.4797951632973229, 'eval_precision': 0.6239760009230414, 'eval_recall': 0.760298045831576, 'eval_roc_auc': 0.8713111526825159, 'eval_accuracy': 0.8989816700610997, 'eval_hamming loss': 0.025923024701028776, 'eval_runtime': 675.3406, 'eval_samples_per_second': 47.258, 'eval_steps_per_second': 2.954, 'epoch': 0.16}
{'loss': 0.2003, 'learning_rate': 1.937331578617535e-05, 'epoch': 0.31}
Best threshold: 0.39999999999999997
                       precision    recall  f1-score   support

label_identity_attack       0.23      0.62      0.34       280
         label_insult       0.58      0.82      0.68      1618
        label_obscene       0.75      0.62      0.68      1705
label_severe_toxicity       0.38      0.24      0.29       318
         label_threat       0.07      0.03      0.04       114
       label_toxicity       0.69      0.80      0.74      3078

            micro avg       0.62      0.72      0.66      7113
            macro avg       0.45      0.52      0.46      7113
         weighted avg       0.64      0.72      0.66      7113
          samples avg       0.06      0.07      0.06      7113

{'eval_loss': 0.24467667937278748, 'eval_f1': 0.6635794416905817, 'eval_f1_macro': 0.46198865380793985, 'eval_precision': 0.6189317435211096, 'eval_recall': 0.7151694081259665, 'eval_roc_auc': 0.8490912368734748, 'eval_accuracy': 0.8903023656587812, 'eval_hamming loss': 0.026936132435114106, 'eval_runtime': 674.2158, 'eval_samples_per_second': 47.336, 'eval_steps_per_second': 2.959, 'epoch': 0.31}
{'loss': 0.2005, 'learning_rate': 1.9059973679263022e-05, 'epoch': 0.47}
Best threshold: 0.3
                       precision    recall  f1-score   support

label_identity_attack       0.22      0.31      0.26       280
         label_insult       0.49      0.90      0.64      1618
        label_obscene       0.53      0.91      0.67      1705
label_severe_toxicity       0.30      0.80      0.43       318
         label_threat       0.52      0.14      0.22       114
       label_toxicity       0.59      0.91      0.72      3078

            micro avg       0.52      0.87      0.65      7113
            macro avg       0.44      0.66      0.49      7113
         weighted avg       0.53      0.87      0.65      7113
          samples avg       0.07      0.09      0.07      7113

{'eval_loss': 0.17280085384845734, 'eval_f1': 0.6490914819871864, 'eval_f1_macro': 0.4896363760848561, 'eval_precision': 0.518065219213681, 'eval_recall': 0.8688317165752847, 'eval_roc_auc': 0.9188255189286116, 'eval_accuracy': 0.8665831113896287, 'eval_hamming loss': 0.034894772572980313, 'eval_runtime': 675.9487, 'eval_samples_per_second': 47.215, 'eval_steps_per_second': 2.951, 'epoch': 0.47}
{'loss': 0.1728, 'learning_rate': 1.8746631572350692e-05, 'epoch': 0.63}
Best threshold: 0.49999999999999994
                       precision    recall  f1-score   support

label_identity_attack       0.38      0.45      0.41       280
         label_insult       0.62      0.83      0.71      1618
        label_obscene       0.69      0.86      0.77      1705
label_severe_toxicity       0.37      0.46      0.41       318
         label_threat       0.85      0.10      0.17       114
       label_toxicity       0.85      0.73      0.79      3078

            micro avg       0.70      0.75      0.72      7113
            macro avg       0.63      0.57      0.54      7113
         weighted avg       0.72      0.75      0.72      7113
          samples avg       0.06      0.07      0.06      7113

{'eval_loss': 0.2192453294992447, 'eval_f1': 0.7235481035764991, 'eval_f1_macro': 0.5429144031589758, 'eval_precision': 0.6969262828861683, 'eval_recall': 0.7522845494165612, 'eval_roc_auc': 0.869831834685935, 'eval_accuracy': 0.9138962870123766, 'eval_hamming loss': 0.02135359548801504, 'eval_runtime': 675.3665, 'eval_samples_per_second': 47.256, 'eval_steps_per_second': 2.954, 'epoch': 0.63}
{'loss': 0.1744, 'learning_rate': 1.8433289465438365e-05, 'epoch': 0.78}
Best threshold: 0.39999999999999997
                       precision    recall  f1-score   support

label_identity_attack       0.42      0.50      0.46       280
         label_insult       0.60      0.87      0.71      1618
        label_obscene       0.65      0.88      0.75      1705
label_severe_toxicity       0.31      0.70      0.43       318
         label_threat       0.38      0.52      0.44       114
       label_toxicity       0.73      0.85      0.79      3078

            micro avg       0.63      0.83      0.72      7113
            macro avg       0.52      0.72      0.60      7113
         weighted avg       0.65      0.83      0.73      7113
          samples avg       0.06      0.08      0.07      7113

{'eval_loss': 0.1498577743768692, 'eval_f1': 0.7185288246325087, 'eval_f1_macro': 0.5954117780244744, 'eval_precision': 0.630600976852835, 'eval_recall': 0.8349500913819766, 'eval_roc_auc': 0.9080405717598582, 'eval_accuracy': 0.9010809963966787, 'eval_hamming loss': 0.02429891900360332, 'eval_runtime': 675.6096, 'eval_samples_per_second': 47.239, 'eval_steps_per_second': 2.953, 'epoch': 0.78}
{'loss': 0.1887, 'learning_rate': 1.811994735852604e-05, 'epoch': 0.94}
Best threshold: 0.5999999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.33      0.40      0.36       280
         label_insult       0.66      0.81      0.73      1618
        label_obscene       0.70      0.83      0.76      1705
label_severe_toxicity       0.26      0.68      0.38       318
         label_threat       1.00      0.02      0.03       114
       label_toxicity       0.86      0.73      0.79      3078

            micro avg       0.68      0.75      0.71      7113
            macro avg       0.63      0.58      0.51      7113
         weighted avg       0.73      0.75      0.72      7113
          samples avg       0.06      0.07      0.06      7113

{'eval_loss': 0.18632102012634277, 'eval_f1': 0.7109652354477861, 'eval_f1_macro': 0.5079558188916301, 'eval_precision': 0.6789918116683725, 'eval_recall': 0.7460986925347954, 'eval_roc_auc': 0.866245352276824, 'eval_accuracy': 0.9117656274479085, 'eval_hamming loss': 0.0225338137761763, 'eval_runtime': 676.7703, 'eval_samples_per_second': 47.158, 'eval_steps_per_second': 2.948, 'epoch': 0.94}
{'loss': 0.152, 'learning_rate': 1.7806605251613712e-05, 'epoch': 1.1}
Best threshold: 0.6499999999999999
                       precision    recall  f1-score   support

label_identity_attack       0.42      0.55      0.48       280
         label_insult       0.77      0.62      0.69      1618
        label_obscene       0.76      0.79      0.77      1705
label_severe_toxicity       0.30      0.74      0.43       318
         label_threat       0.38      0.50      0.43       114
       label_toxicity       0.85      0.74      0.79      3078

            micro avg       0.72      0.72      0.72      7113
            macro avg       0.58      0.66      0.60      7113
         weighted avg       0.76      0.72      0.73      7113
          samples avg       0.06      0.06      0.06      7113

{'eval_loss': 0.15077145397663116, 'eval_f1': 0.7168322412456846, 'eval_f1_macro': 0.5979943522704381, 'eval_precision': 0.718502824858757, 'eval_recall': 0.7151694081259665, 'eval_roc_auc': 0.8521800169273861, 'eval_accuracy': 0.9125802913990286, 'eval_hamming loss': 0.02098804115097394, 'eval_runtime': 676.8487, 'eval_samples_per_second': 47.152, 'eval_steps_per_second': 2.947, 'epoch': 1.1}
{'train_runtime': 12553.7996, 'train_samples_per_second': 101.687, 'train_steps_per_second': 12.711, 'train_loss': 0.18759546595982143, 'epoch': 1.1}
Job ID: 14735290
Cluster: puhti
User/Group: annieske/annieske
State: RUNNING
Cores: 1
CPU Utilized: 03:29:27
CPU Efficiency: 99.09% of 03:31:23 core-walltime
Job Wall-clock time: 03:31:23
Memory Utilized: 6.09 GB
Memory Efficiency: 77.93% of 7.81 GB
Job consumed 217.66 CSC billing units based on following used resources
Billed project: project_2000539
CPU BU: 3.52
Mem BU: 2.75
GPU BU: 211.38
GPU job efficiency:
------------------------------------------------------------------------
Host memory 
     Hostname    Mean (GiB)  stdDev (GiB)     Max (GiB) 
       r15g06         13.69          1.12         13.81 
------------------------------------------------------------------------
GPU load 
     Hostname        GPU Id      Mean (%)    stdDev (%)       Max (%) 
       r15g06             2 [32m        95.35 [0m        12.52           100 
------------------------------------------------------------------------
GPU memory 
     Hostname        GPU Id    Mean (GiB)  stdDev (GiB)     Max (GiB) 
       r15g06             2         24.72          2.46         25.07 
------------------------------------------------------------------------
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
END 14735290: Thu Dec 22 20:38:54 EET 2022
