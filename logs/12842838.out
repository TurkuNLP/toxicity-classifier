epochs: 4, learning rate: 8e-6, batch size: 8, prediction treshold: 0.6, model: xlm-roberta-base 
Translated train and test
Namespace(batch=8, binary=True, clean_as_label=True, dev=False, epochs=4, learning=8e-06, loss=True, model='xlm-roberta-base', test='data/test_fi_deepl.jsonl', threshold=None, train=['data/train_fi_deepl.jsonl'])
['data/train_fi_deepl.jsonl']
tensor([16.2248,  2.8940,  2.6981, 14.2921, 47.6901,  1.4905,  0.1590],
       device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 159571
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.215, 'learning_rate': 6e-06, 'epoch': 1.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.2124617099761963, 'eval_accuracy': 0.9303510581762481, 'eval_roc_auc': 0.8644801366930807, 'eval_f1': 0.6868147315153219, 'eval_precision': 0.6118973074514715, 'eval_recall': 0.782636552939292, 'eval_runtime': 537.8193, 'eval_samples_per_second': 118.958, 'eval_steps_per_second': 3.719, 'epoch': 1.0}
{'loss': 0.1632, 'learning_rate': 4e-06, 'epoch': 2.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.16842055320739746, 'eval_accuracy': 0.9236925193035106, 'eval_roc_auc': 0.8829339938933736, 'eval_f1': 0.6803718737724237, 'eval_precision': 0.5753515668253792, 'eval_recall': 0.8322921672272946, 'eval_runtime': 538.4781, 'eval_samples_per_second': 118.813, 'eval_steps_per_second': 3.714, 'epoch': 2.0}
{'loss': 0.1329, 'learning_rate': 2e-06, 'epoch': 3.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.1687535047531128, 'eval_accuracy': 0.9230673043858827, 'eval_roc_auc': 0.8849447551691774, 'eval_f1': 0.6799739921976593, 'eval_precision': 0.5722884973185948, 'eval_recall': 0.8375780874579529, 'eval_runtime': 540.219, 'eval_samples_per_second': 118.43, 'eval_steps_per_second': 3.702, 'epoch': 3.0}
{'loss': 0.1167, 'learning_rate': 0.0, 'epoch': 4.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.18135367333889008, 'eval_accuracy': 0.9215511582106349, 'eval_roc_auc': 0.8867475995042221, 'eval_f1': 0.6772554819625747, 'eval_precision': 0.565749892565535, 'eval_recall': 0.8435047252923275, 'eval_runtime': 539.9192, 'eval_samples_per_second': 118.496, 'eval_steps_per_second': 3.704, 'epoch': 4.0}
END: pe 19.8.2022 20.13.02 +0300
