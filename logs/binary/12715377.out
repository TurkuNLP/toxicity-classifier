epochs: 4, learning rate: 8e-6, batch size: 8, model: TurkuNLP/bert-base-finnish-cased-v1 
Translated train and test
Namespace(batch=8, dev=False, epochs=4, learning=8e-06, loss=True, model='TurkuNLP/bert-base-finnish-cased-v1', test='data/test_fi_deepl.jsonl', train='data/train_fi_deepl.jsonl')
toxic:  16225
clean:  143346
toxic:  6243
clean:  57735
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 200
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 200
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1076, 'learning_rate': 6e-06, 'epoch': 1.0}
[[0.10420103]
 [0.15410276]
 [0.09107302]
 [0.02740785]
 [0.16555643]]
tensor([[0.5260],
        [0.5384],
        [0.5228],
        [0.5069],
        [0.5413]])
{'eval_loss': 0.07073144614696503, 'eval_accuracy': 0.92, 'eval_weighted_accuracy': 0.4972972972972973, 'eval_roc_auc': 0.4972972972972973, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 1.6765, 'eval_samples_per_second': 119.295, 'eval_steps_per_second': 4.175, 'epoch': 1.0}
{'loss': 0.074, 'learning_rate': 4e-06, 'epoch': 2.0}
[[0.15957887]
 [0.16247945]
 [0.22085978]
 [0.1039416 ]
 [0.15633084]]
tensor([[0.5398],
        [0.5405],
        [0.5550],
        [0.5260],
        [0.5390]])
{'eval_loss': 0.07082559913396835, 'eval_accuracy': 0.925, 'eval_weighted_accuracy': 0.6225225225225225, 'eval_roc_auc': 0.6225225225225225, 'eval_f1': 0.3478260869565218, 'eval_precision': 0.5, 'eval_recall': 0.26666666666666666, 'eval_runtime': 1.6636, 'eval_samples_per_second': 120.223, 'eval_steps_per_second': 4.208, 'epoch': 2.0}
{'loss': 0.0536, 'learning_rate': 2e-06, 'epoch': 3.0}
[[ 0.01093866]
 [ 0.06478903]
 [ 0.1366582 ]
 [-0.08536501]
 [-0.02202727]]
tensor([[0.5027],
        [0.5162],
        [0.5341],
        [0.4787],
        [0.4945]])
{'eval_loss': 0.0702889934182167, 'eval_accuracy': 0.925, 'eval_weighted_accuracy': 0.5306306306306307, 'eval_roc_auc': 0.5306306306306307, 'eval_f1': 0.11764705882352941, 'eval_precision': 0.5, 'eval_recall': 0.06666666666666667, 'eval_runtime': 1.6653, 'eval_samples_per_second': 120.099, 'eval_steps_per_second': 4.203, 'epoch': 3.0}
{'loss': 0.042, 'learning_rate': 0.0, 'epoch': 4.0}
[[0.09807683]
 [0.14390935]
 [0.23470512]
 [0.01020337]
 [0.06951692]]
tensor([[0.5245],
        [0.5359],
        [0.5584],
        [0.5026],
        [0.5174]])
{'eval_loss': 0.0647478848695755, 'eval_accuracy': 0.92, 'eval_weighted_accuracy': 0.5891891891891892, 'eval_roc_auc': 0.5891891891891892, 'eval_f1': 0.27272727272727276, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.2, 'eval_runtime': 1.6724, 'eval_samples_per_second': 119.586, 'eval_steps_per_second': 4.186, 'epoch': 4.0}
{'train_runtime': 37.5361, 'train_samples_per_second': 21.313, 'train_steps_per_second': 2.664, 'train_loss': 0.06930204272270203, 'epoch': 4.0}
[[0.09807683]
 [0.14390935]
 [0.23470512]
 [0.01020337]
 [0.06951692]]
tensor([[0.5245],
        [0.5359],
        [0.5584],
        [0.5026],
        [0.5174]])
F1_micro: 0.0
[[0.09807683]
 [0.14390935]
 [0.23470512]
 [0.01020337]
 [0.06951692]]
tensor([[0.5245],
        [0.5359],
        [0.5584],
        [0.5026],
        [0.5174]])
              precision    recall  f1-score   support

       clean       0.92      1.00      0.96        92
       toxic       0.00      0.00      0.00         8

    accuracy                           0.92       100
   macro avg       0.46      0.50      0.48       100
weighted avg       0.85      0.92      0.88       100

[[0.09807683]
 [0.14390935]
 [0.23470512]
 [0.01020337]
 [0.06951692]]
tensor([[0.5245],
        [0.5359],
        [0.5584],
        [0.5026],
        [0.5174]])
[[0.09807683]
 [0.14390935]
 [0.23470512]
 [0.01020337]
 [0.06951692]]
tensor([[0.5245],
        [0.5359],
        [0.5584],
        [0.5026],
        [0.5174]])
[('== HOPI ASS HOLES == \n\n HOPI-KANSA ON TÄYSIÄ KUSIPÄITÄ!', 'toxic',
  tensor([0.6553])),
 ('= HALPAA PROPAGANDAA WIKIPEDIASSA KREIKAN OLEMASSAOLEMATTOMIEN '
  'VÄHEMMISTÖJEN PUOLESTA = \n'
  ' = VITUN WIKIPEDIA YRITTÄÄ LUODA ONGELMIA ..!!!! = \n'
  ' = WIKIPEDIA ON VALEHTELEVA ORGANISAATIO, JOKA PERUSTUU PROPAGANDAAN = \n'
  ' = WIKIPEDIA - VALHEEN MÄÄRITELMÄ = \n'
  ' = WIKIPEDIA - MÄÄRITELMÄ VÄÄRÄN TIEDON TARKOITUS = \n'
  ' = VITUN WIKIPEDIA ON ROSKALEHTI, JOTA KENENKÄÄN EI TARVITSE OTTAA '
  'VAKAVASTI ! = \n'
  '\n'
  '  \n'
  ' = HALPAA PROPAGANDAA WIKIPEDIASSA KREIKAN OLEMASSAOLEMATTOMIEN '
  'VÄHEMMISTÖJEN PUOLESTA = \n'
  ' = VITUN WIKIPEDIA YRITTÄÄ LUODA ONGELMIA ..!!!! = \n'
  ' = WIKIPEDIA ON VALEHTELEVA ORGANISAATIO, JOKA PERUSTUU PROPAGANDAAN = \n'
  ' = WIKIPEDIA - VALHEEN MÄÄRITELMÄ = \n'
  ' = WIKIPEDIA - MÄÄRITELMÄ VÄÄRÄN TIEDON TARKOITUS = \n'
  ' = VITUN WIKIPEDIA ON ROSKALEHTI, JOTA KENENKÄÄN EI TARVITSE OTTAA '
  'VAKAVASTI ! = \n'
  '\n'
  '  \n'
  ' = HALPAA PROPAGANDAA WIKIPEDIASSA KREIKAN OLEMASSAOLEMATTOMIEN '
  'VÄHEMMISTÖJEN PUOLESTA = \n'
  ' = VITUN WIKIPEDIA YRITTÄÄ LUODA ONGELMIA ..!!!! = \n'
  ' = WIKIPEDIA ON VALEHTELEVA ORGANISAATIO, JOKA PERUSTUU PROPAGANDAAN = \n'
  ' = WIKIPEDIA - VALHEEN MÄÄRITELMÄ = \n'
  ' = WIKIPEDIA - MÄÄRITELMÄ VÄÄRÄN TIEDON TARKOITUS = \n'
  ' = VITUN WIKIPEDIA ON ROSKALEHTI, JOTA KENENKÄÄN EI TARVITSE OTTAA '
  'VAKAVASTI ! = \n'
  '\n'
  '  \n'
  '\n'
  ' = HALPAA PROPAGANDAA WIKIPEDIASSA KREIKAN OLEMASSAOLEMATTOMIEN '
  'VÄHEMMISTÖJEN PUOLESTA = \n'
  ' = VITUN WIKIPEDIA YRITTÄÄ LUODA ONGELMIA ..!!!! = \n'
  ' = WIKIPEDIA ON VALEHTELEVA ORGANISAATIO, JOKA PERUSTUU PROPAGANDAAN = \n'
  ' = WIKIPEDIA - VALHEEN MÄÄRITELMÄ = \n'
  ' = WIKIPEDIA - MÄÄRITELMÄ VÄÄRÄN TIEDON TARKOITUS = \n'
  ' = VITUN WIKIPEDIA ON ROSKALEHTI, JOTA KENENKÄÄN EI TARVITSE OTTAA '
  'VAKAVASTI ! =',
  'toxic', tensor([0.6378])),
 ('== kiitos! == \n\n kiitos paljon, että poistit sivuni, mulkku.', 'toxic',
  tensor([0.6180])),
 ('KUKA HELVETTI SINÄ OLET HERRA OLEN NIIN TÄRKEÄ HENKILÖ, JOKA TIETÄÄ KAIKEN '
  'WIKIPEDIASTA?',
  'toxic', tensor([0.6121])),
 ('Hei im Jenise \n olen luuseri', 'toxic', tensor([0.6083]))]
[('== kiitos! == \n\n kiitos paljon, että poistit sivuni, mulkku.', 'toxic',
  tensor([0.6180])),
 ('KUKA HELVETTI SINÄ OLET HERRA OLEN NIIN TÄRKEÄ HENKILÖ, JOKA TIETÄÄ KAIKEN '
  'WIKIPEDIASTA?',
  'toxic', tensor([0.6121])),
 ('Hei im Jenise \n olen luuseri', 'toxic', tensor([0.6083])),
 ('=="Hei, typerys!" "Hei, typerys! \n'
  ' Olet maailman pahin mulkku. plkese korjaa muokkaukset oldthew on vanha, '
  'joten ajattelin kertoa hänelle sen. olet myös tyhmä ja mulkku. rakkaus, '
  'varjeiiiina heda.',
  'toxic', tensor([0.6011])),
 ('Todennäköisesti me kaikki kuolemme vuonna 2012.', 'toxic', tensor([0.6005]))]
[('ronallololololoktrololol0l0 sinua on trollailtu', 'clean', tensor([0.6000])), ('Wikinerds sai peepeesinsä pilkata, se ei nessecitate lohko. Ole hyvä ja poista pää peräaukosta, kiitos.', 'clean', tensor([0.5963])), ('YERRR JA WAH MITÄ TEIT SITTEN???', 'clean', tensor([0.5935])), ('Voin postata mitä vittua haluan. Tämä on internet, joten menkää pois, NSA:n kätyrit. Voin julkaista artikkeleita, jotka liittyvät ihmisiin MMORPG:ssä nimeltä Supremacy1914. \n\n Jos olet eri mieltä, niin fubar. \n\n Kastelen kalkkunan ja harrastan sen kanssa seksiä. \n Aivan oikein, aion naida pelkoa Turkkia.', 'clean', tensor([0.5903])), ('== Gobbledee Gook == \n\n Hei, anna minun tappaa sinut, me hyväksymme eri imamoja!', 'clean', tensor([0.5894]))]
END: to 11.8.2022 15.17.02 +0300
