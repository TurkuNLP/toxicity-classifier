epochs: 4, learning rate: 2e-5, batch size: 8, model: TurkuNLP/bert-base-finnish-cased-v1 
Translated train and test
Namespace(batch=8, dev=True, epochs=4, learning=2e-05, model='TurkuNLP/bert-base-finnish-cased-v1', test='data/test_fi_deepl.jsonl', train='data/train_fi_deepl.jsonl')
toxic:  16225
clean:  143346
toxic:  6243
clean:  57735
tensor([0.5566, 4.9174], device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 127656
    })
    dev: Dataset({
        features: ['text', 'labels'],
        num_rows: 31915
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1648, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.0}
{'eval_loss': 0.18169018626213074, 'eval_accuracy': 0.9578881403728654, 'eval_weighted_accuracy': 0.8797892853502423, 'eval_f1': 0.7921435199505104, 'eval_precision': 0.8030730636563186, 'eval_recall': 0.7815074763503204, 'eval_runtime': 261.9936, 'eval_samples_per_second': 121.816, 'eval_steps_per_second': 3.809, 'epoch': 1.0}
{'loss': 0.138, 'learning_rate': 1e-05, 'epoch': 2.0}
{'eval_loss': 0.1377003788948059, 'eval_accuracy': 0.960081466395112, 'eval_weighted_accuracy': 0.8926316939389021, 'eval_f1': 0.8060292326431182, 'eval_precision': 0.8043147979337587, 'eval_recall': 0.8077509917607568, 'eval_runtime': 261.9396, 'eval_samples_per_second': 121.841, 'eval_steps_per_second': 3.81, 'epoch': 2.0}
{'loss': 0.1121, 'learning_rate': 5e-06, 'epoch': 3.0}
{'eval_loss': 0.17132581770420074, 'eval_accuracy': 0.960206799310669, 'eval_weighted_accuracy': 0.9051325029708548, 'eval_f1': 0.8117960877296976, 'eval_precision': 0.7891097666378565, 'eval_recall': 0.835825450106805, 'eval_runtime': 261.7308, 'eval_samples_per_second': 121.938, 'eval_steps_per_second': 3.813, 'epoch': 3.0}
{'loss': 0.0852, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.16959315538406372, 'eval_accuracy': 0.9611467961773461, 'eval_weighted_accuracy': 0.9010622277861883, 'eval_f1': 0.8135338345864661, 'eval_precision': 0.801956715090424, 'eval_recall': 0.8254501068050045, 'eval_runtime': 261.7827, 'eval_samples_per_second': 121.914, 'eval_steps_per_second': 3.812, 'epoch': 4.0}
{'train_runtime': 10002.1926, 'train_samples_per_second': 51.051, 'train_steps_per_second': 6.381, 'train_loss': 0.12499526467860891, 'epoch': 4.0}
F1_micro: 0.6512221816855163
              precision    recall  f1-score   support

       clean       0.98      0.92      0.95     57735
       toxic       0.52      0.86      0.65      6243

    accuracy                           0.91     63978
   macro avg       0.75      0.89      0.80     63978
weighted avg       0.94      0.91      0.92     63978

END: ti 2.8.2022 12.29.02 +0300
