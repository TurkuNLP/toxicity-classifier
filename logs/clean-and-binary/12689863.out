epochs: 4, learning rate: 8e-6, batch size: 8, prediction treshold: 0.6, model: xlm-roberta-base 
transfer from english train to translated finnish test
Namespace(batch=8, binary=True, clean_as_label=True, dev=False, epochs=4, learning=8e-06, loss=True, model='xlm-roberta-base', test='data/test_fi_deepl.jsonl', threshold=0.6, train='data/train_en.jsonl')
tensor([16.2248,  2.8940,  2.6981, 14.2921, 47.6901,  1.4905,  0.1590],
       device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 159571
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1829, 'learning_rate': 6e-06, 'epoch': 1.0}
{'eval_loss': 0.3647546172142029, 'eval_accuracy': 0.921644940448279, 'eval_roc_auc': 0.7421549402152962, 'eval_f1': 0.5638973466724663, 'eval_precision': 0.6170982482863671, 'eval_recall': 0.5191414384110203, 'eval_runtime': 546.6622, 'eval_samples_per_second': 117.034, 'eval_steps_per_second': 3.659, 'epoch': 1.0}
{'loss': 0.1292, 'learning_rate': 4e-06, 'epoch': 2.0}
{'eval_loss': 0.33518895506858826, 'eval_accuracy': 0.9224889805870768, 'eval_roc_auc': 0.768837199508084, 'eval_f1': 0.5926899383983574, 'eval_precision': 0.6082265677680377, 'eval_recall': 0.5779272785519782, 'eval_runtime': 546.829, 'eval_samples_per_second': 116.998, 'eval_steps_per_second': 3.657, 'epoch': 2.0}
{'loss': 0.106, 'learning_rate': 2e-06, 'epoch': 3.0}
{'eval_loss': 0.39046820998191833, 'eval_accuracy': 0.9230360436400012, 'eval_roc_auc': 0.7773546944154486, 'eval_f1': 0.6019401778496363, 'eval_precision': 0.6076383221805125, 'eval_recall': 0.5963479096588179, 'eval_runtime': 545.7286, 'eval_samples_per_second': 117.234, 'eval_steps_per_second': 3.665, 'epoch': 3.0}
{'loss': 0.0908, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.40015098452568054, 'eval_accuracy': 0.9166275907343149, 'eval_roc_auc': 0.7891613187180138, 'eval_f1': 0.5962149886449659, 'eval_precision': 0.5652361131046362, 'eval_recall': 0.6307864808585616, 'eval_runtime': 545.5304, 'eval_samples_per_second': 117.277, 'eval_steps_per_second': 3.666, 'epoch': 4.0}
{'train_runtime': 15561.8323, 'train_samples_per_second': 41.016, 'train_steps_per_second': 5.127, 'train_loss': 0.12722997712737583, 'epoch': 4.0}
F1: 0.5926899383983574
              precision    recall  f1-score   support

       clean       0.95      0.96      0.96     57735
       toxic       0.61      0.58      0.59      6243

    accuracy                           0.92     63978
   macro avg       0.78      0.77      0.77     63978
weighted avg       0.92      0.92      0.92     63978

                       precision    recall  f1-score   support

label_identity_attack       0.46      0.51      0.48       712
         label_insult       0.62      0.45      0.52      3427
        label_obscene       0.62      0.39      0.48      3691
label_severe_toxicity       0.25      0.31      0.28       367
         label_threat       0.36      0.27      0.31       211
       label_toxicity       0.60      0.58      0.59      6090

            micro avg       0.58      0.49      0.53     14498
            macro avg       0.48      0.42      0.44     14498
         weighted avg       0.59      0.49      0.53     14498
          samples avg       0.05      0.04      0.04     14498

END: ma 8.8.2022 20.10.22 +0300
