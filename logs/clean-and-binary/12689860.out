epochs: 4, learning rate: 2e-5, batch size: 8, prediction treshold: 0.6, model: TurkuNLP/bert-base-finnish-cased-v1 
Translated train and test
Namespace(batch=8, binary=True, clean_as_label=True, dev=False, epochs=4, learning=2e-05, loss=True, model='TurkuNLP/bert-base-finnish-cased-v1', test='data/test_fi_deepl.jsonl', threshold=0.6, train='data/train_fi_deepl.jsonl')
tensor([16.2248,  2.8940,  2.6981, 14.2921, 47.6901,  1.4905,  0.1590],
       device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 159571
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1752, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.0}
{'eval_loss': 0.17789433896541595, 'eval_accuracy': 0.9366032073525274, 'eval_roc_auc': 0.8475868488425405, 'eval_f1': 0.6940715039975864, 'eval_precision': 0.655880256593015, 'eval_recall': 0.7369854236745155, 'eval_runtime': 529.6719, 'eval_samples_per_second': 120.788, 'eval_steps_per_second': 3.776, 'epoch': 1.0}
{'loss': 0.135, 'learning_rate': 1e-05, 'epoch': 2.0}
{'eval_loss': 0.1632215529680252, 'eval_accuracy': 0.9052174184876051, 'eval_roc_auc': 0.9002693336099954, 'eval_f1': 0.6480148595309961, 'eval_precision': 0.5081474738279472, 'eval_recall': 0.8941214159859042, 'eval_runtime': 530.0371, 'eval_samples_per_second': 120.705, 'eval_steps_per_second': 3.773, 'epoch': 2.0}
{'loss': 0.0992, 'learning_rate': 5e-06, 'epoch': 3.0}
{'eval_loss': 0.17901374399662018, 'eval_accuracy': 0.914548751133202, 'eval_roc_auc': 0.8995822934052987, 'eval_f1': 0.6680026720106881, 'eval_precision': 0.5379499217527387, 'eval_recall': 0.8809867051097229, 'eval_runtime': 529.7574, 'eval_samples_per_second': 120.768, 'eval_steps_per_second': 3.775, 'epoch': 3.0}
{'loss': 0.0732, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.20834200084209442, 'eval_accuracy': 0.9180186939260371, 'eval_roc_auc': 0.8900047304735006, 'eval_f1': 0.6706022734409345, 'eval_precision': 0.5515495867768595, 'eval_recall': 0.8551978215601473, 'eval_runtime': 529.9179, 'eval_samples_per_second': 120.732, 'eval_steps_per_second': 3.774, 'epoch': 4.0}
{'train_runtime': 13852.0144, 'train_samples_per_second': 46.079, 'train_steps_per_second': 5.76, 'train_loss': 0.12066964638910299, 'epoch': 4.0}
F1: 0.6480148595309961
              precision    recall  f1-score   support

       clean       0.99      0.91      0.95     57735
       toxic       0.51      0.89      0.65      6243

    accuracy                           0.91     63978
   macro avg       0.75      0.90      0.80     63978
weighted avg       0.94      0.91      0.92     63978

                       precision    recall  f1-score   support

label_identity_attack       0.40      0.74      0.52       712
         label_insult       0.55      0.79      0.65      3427
        label_obscene       0.53      0.83      0.64      3691
label_severe_toxicity       0.21      0.84      0.34       367
         label_threat       0.36      0.66      0.47       211
       label_toxicity       0.50      0.90      0.64      6090

            micro avg       0.49      0.84      0.62     14498
            macro avg       0.42      0.79      0.54     14498
         weighted avg       0.50      0.84      0.63     14498
          samples avg       0.07      0.08      0.07     14498

END: ma 8.8.2022 19.39.43 +0300
