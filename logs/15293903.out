START ma 6.2.2023 11.07.28 +0200
Namespace(model='models/finbert-large-deepl', data='data/test_fi_deepl.jsonl', tokenizer='TurkuNLP/bert-base-finnish-cased-v1', filename='test3.tsv', new_test=True, threshold=0.6499999999999999)
                                                text  ...              labels
0  Persujen mamu-puputus kyllästyttää  mutta mamu...  ...  [0, 0, 0, 0, 0, 1]
1  Suvakit ja hyysärit haluaa saada lapset omaan ...  ...  [0, 0, 0, 0, 0, 1]
2  VIELÄ YKSI ASIA! NÄMÄ EI OLE HENKILÖVAALIT,VAA...  ...  [0, 0, 0, 0, 0, 0]
3  Halosen aikana on lapsiin ja naisiin kohdistun...  ...  [0, 0, 0, 0, 0, 1]
4  Byää! Byää! Jos tulee vammoja, byää! Saatanan ...  ...  [0, 0, 0, 0, 0, 1]

[5 rows x 3 columns]
Dataset({
    features: ['text', 'id', 'labels'],
    num_rows: 2252
})
tensor([[1.1840e-01, 5.8340e-02, 1.4804e-01, 3.0689e-03, 5.7892e-03, 9.3099e-01],
        [8.5944e-02, 3.9587e-02, 1.2726e-01, 3.2833e-03, 1.2636e-02, 9.2524e-01],
        [1.3314e-03, 3.4000e-03, 7.9556e-03, 1.6218e-04, 1.7250e-03, 6.3318e-02],
        [4.3667e-02, 1.0461e-01, 1.8257e-01, 5.1093e-03, 8.1963e-03, 9.8091e-01],
        [9.9302e-01, 9.9851e-01, 9.9841e-01, 7.2457e-01, 5.2963e-02, 9.9922e-01],
        [3.0064e-04, 1.1481e-03, 2.7582e-03, 6.5535e-05, 4.8457e-04, 2.0657e-02],
        [1.1050e-02, 2.5606e-02, 1.0061e-01, 1.0591e-03, 4.6801e-03, 5.2831e-01],
        [3.4559e-03, 5.7862e-02, 9.5298e-01, 1.1666e-02, 4.2714e-03, 9.7839e-01],
        [8.4953e-03, 2.8285e-02, 1.0968e-01, 9.5429e-04, 3.1840e-03, 4.8982e-01],
        [7.8740e-04, 2.4626e-03, 5.4028e-03, 1.1414e-04, 1.1117e-03, 4.0459e-02]])
[[0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 ...
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]]
tensor([[1.1840e-01, 5.8340e-02, 1.4804e-01, 3.0689e-03, 5.7892e-03, 9.3099e-01],
        [8.5944e-02, 3.9587e-02, 1.2726e-01, 3.2833e-03, 1.2636e-02, 9.2524e-01],
        [1.3314e-03, 3.4000e-03, 7.9556e-03, 1.6218e-04, 1.7250e-03, 6.3318e-02],
        [4.3667e-02, 1.0461e-01, 1.8257e-01, 5.1093e-03, 8.1963e-03, 9.8091e-01],
        [9.9302e-01, 9.9851e-01, 9.9841e-01, 7.2457e-01, 5.2963e-02, 9.9922e-01],
        [3.0064e-04, 1.1481e-03, 2.7582e-03, 6.5535e-05, 4.8457e-04, 2.0657e-02],
        [1.1050e-02, 2.5606e-02, 1.0061e-01, 1.0591e-03, 4.6801e-03, 5.2831e-01],
        [3.4559e-03, 5.7862e-02, 9.5298e-01, 1.1666e-02, 4.2714e-03, 9.7839e-01],
        [8.4953e-03, 2.8285e-02, 1.0968e-01, 9.5429e-04, 3.1840e-03, 4.8982e-01],
        [7.8740e-04, 2.4626e-03, 5.4028e-03, 1.1414e-04, 1.1117e-03, 4.0459e-02],
        [2.3153e-04, 7.6717e-04, 1.8308e-03, 6.4507e-05, 4.7652e-04, 1.2835e-02],
        [7.5299e-04, 2.2751e-03, 5.1159e-03, 1.1101e-04, 1.1013e-03, 3.8370e-02],
        [1.2145e-02, 1.0133e-01, 4.3918e-01, 4.7401e-03, 5.8265e-03, 9.6724e-01],
        [5.2549e-02, 9.9673e-01, 9.9422e-01, 5.4483e-02, 1.3416e-02, 9.9800e-01],
        [1.2601e-02, 1.2079e-01, 4.1746e-01, 5.2426e-03, 6.7586e-03, 9.7791e-01],
        [1.6833e-04, 4.7275e-04, 1.1108e-03, 7.4856e-05, 3.5607e-04, 7.4998e-03],
        [1.7595e-02, 5.4628e-02, 2.2218e-01, 2.6679e-03, 7.1091e-03, 8.8283e-01],
        [1.6769e-02, 5.5754e-02, 1.8045e-01, 3.0243e-03, 1.0789e-02, 9.4087e-01],
        [2.3110e-04, 7.6603e-04, 1.8333e-03, 6.4573e-05, 4.7791e-04, 1.2838e-02],
        [3.9925e-01, 6.5319e-01, 9.6449e-01, 1.8230e-02, 4.1518e-03, 9.9205e-01],
        [1.9103e-02, 2.8545e-02, 1.0896e-01, 1.3445e-03, 5.1196e-03, 6.1394e-01],
        [1.6990e-02, 6.3727e-02, 4.4615e-01, 8.4985e-03, 8.8691e-03, 9.8658e-01],
        [3.6052e-03, 7.8018e-03, 2.0645e-02, 3.5055e-04, 4.0958e-03, 1.8517e-01],
        [1.0782e-02, 2.4858e-02, 7.4568e-02, 8.6006e-04, 3.6202e-03, 4.7710e-01],
        [1.4044e-02, 5.9392e-02, 1.2804e-01, 1.2697e-03, 3.8775e-03, 7.3303e-01],
        [4.8016e-03, 1.3546e-02, 3.4415e-02, 4.4217e-04, 2.3949e-03, 2.3065e-01],
        [1.8502e-02, 8.4011e-02, 2.1282e-01, 3.0414e-03, 8.3536e-03, 9.4078e-01],
        [3.7046e-03, 4.9031e-01, 5.9418e-01, 2.9704e-03, 2.3252e-03, 9.7833e-01],
        [8.6604e-03, 5.4315e-02, 1.0441e-01, 7.5629e-04, 2.8981e-03, 6.5202e-01],
        [8.2812e-02, 7.9361e-01, 9.8650e-01, 2.1072e-02, 4.2896e-03, 9.9417e-01]])
[[0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]]
[[0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]]
                       precision    recall  f1-score   support

label_identity_attack       1.00      0.29      0.45       131
         label_insult       1.00      0.43      0.60       141
        label_obscene       1.00      0.78      0.87       170
label_severe_toxicity       1.00      0.24      0.39        25
         label_threat       1.00      0.23      0.37        40
       label_toxicity       1.00      0.74      0.85       158

            micro avg       1.00      0.55      0.71       665
            macro avg       1.00      0.45      0.59       665
         weighted avg       1.00      0.55      0.68       665
          samples avg       0.16      0.16      0.16       665

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_accuracy': 0.8658969804618117,
 'eval_f1': 0.7062256809338522,
 'eval_f1_macro': 0.5888649212266751,
 'eval_hamming loss': 0.02235050325636471,
 'eval_loss': 1.3107070922851562,
 'eval_macro_roc_auc': 0.7253897805585215,
 'eval_micro_roc_auc': 0.7729323308270677,
 'eval_precision': 1.0,
 'eval_probs_roc_auc': 0.7003364227113725,
 'eval_recall': 0.5458646616541354,
 'eval_runtime': 74.6655,
 'eval_samples_per_second': 30.161,
 'eval_steps_per_second': 3.777}
tensor([[1.1840e-01, 5.8340e-02, 1.4804e-01, 3.0689e-03, 5.7892e-03, 9.3099e-01],
        [8.5944e-02, 3.9587e-02, 1.2726e-01, 3.2833e-03, 1.2636e-02, 9.2524e-01],
        [1.3314e-03, 3.4000e-03, 7.9556e-03, 1.6218e-04, 1.7250e-03, 6.3318e-02],
        [4.3667e-02, 1.0461e-01, 1.8257e-01, 5.1093e-03, 8.1963e-03, 9.8091e-01],
        [9.9302e-01, 9.9851e-01, 9.9841e-01, 7.2457e-01, 5.2963e-02, 9.9922e-01],
        [3.0064e-04, 1.1481e-03, 2.7582e-03, 6.5535e-05, 4.8457e-04, 2.0657e-02],
        [1.1050e-02, 2.5606e-02, 1.0061e-01, 1.0591e-03, 4.6801e-03, 5.2831e-01],
        [3.4559e-03, 5.7862e-02, 9.5298e-01, 1.1666e-02, 4.2714e-03, 9.7839e-01],
        [8.4953e-03, 2.8285e-02, 1.0968e-01, 9.5429e-04, 3.1840e-03, 4.8982e-01],
        [7.8740e-04, 2.4626e-03, 5.4028e-03, 1.1414e-04, 1.1117e-03, 4.0459e-02]])
[[0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 ...
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]]
tensor([[1.1840e-01, 5.8340e-02, 1.4804e-01, 3.0689e-03, 5.7892e-03, 9.3099e-01],
        [8.5944e-02, 3.9587e-02, 1.2726e-01, 3.2833e-03, 1.2636e-02, 9.2524e-01],
        [1.3314e-03, 3.4000e-03, 7.9556e-03, 1.6218e-04, 1.7250e-03, 6.3318e-02],
        [4.3667e-02, 1.0461e-01, 1.8257e-01, 5.1093e-03, 8.1963e-03, 9.8091e-01],
        [9.9302e-01, 9.9851e-01, 9.9841e-01, 7.2457e-01, 5.2963e-02, 9.9922e-01],
        [3.0064e-04, 1.1481e-03, 2.7582e-03, 6.5535e-05, 4.8457e-04, 2.0657e-02],
        [1.1050e-02, 2.5606e-02, 1.0061e-01, 1.0591e-03, 4.6801e-03, 5.2831e-01],
        [3.4559e-03, 5.7862e-02, 9.5298e-01, 1.1666e-02, 4.2714e-03, 9.7839e-01],
        [8.4953e-03, 2.8285e-02, 1.0968e-01, 9.5429e-04, 3.1840e-03, 4.8982e-01],
        [7.8740e-04, 2.4626e-03, 5.4028e-03, 1.1414e-04, 1.1117e-03, 4.0459e-02],
        [2.3153e-04, 7.6717e-04, 1.8308e-03, 6.4507e-05, 4.7652e-04, 1.2835e-02],
        [7.5299e-04, 2.2751e-03, 5.1159e-03, 1.1101e-04, 1.1013e-03, 3.8370e-02],
        [1.2145e-02, 1.0133e-01, 4.3918e-01, 4.7401e-03, 5.8265e-03, 9.6724e-01],
        [5.2549e-02, 9.9673e-01, 9.9422e-01, 5.4483e-02, 1.3416e-02, 9.9800e-01],
        [1.2601e-02, 1.2079e-01, 4.1746e-01, 5.2426e-03, 6.7586e-03, 9.7791e-01],
        [1.6833e-04, 4.7275e-04, 1.1108e-03, 7.4856e-05, 3.5607e-04, 7.4998e-03],
        [1.7595e-02, 5.4628e-02, 2.2218e-01, 2.6679e-03, 7.1091e-03, 8.8283e-01],
        [1.6769e-02, 5.5754e-02, 1.8045e-01, 3.0243e-03, 1.0789e-02, 9.4087e-01],
        [2.3110e-04, 7.6603e-04, 1.8333e-03, 6.4573e-05, 4.7791e-04, 1.2838e-02],
        [3.9925e-01, 6.5319e-01, 9.6449e-01, 1.8230e-02, 4.1518e-03, 9.9205e-01],
        [1.9103e-02, 2.8545e-02, 1.0896e-01, 1.3445e-03, 5.1196e-03, 6.1394e-01],
        [1.6990e-02, 6.3727e-02, 4.4615e-01, 8.4985e-03, 8.8691e-03, 9.8658e-01],
        [3.6052e-03, 7.8018e-03, 2.0645e-02, 3.5055e-04, 4.0958e-03, 1.8517e-01],
        [1.0782e-02, 2.4858e-02, 7.4568e-02, 8.6006e-04, 3.6202e-03, 4.7710e-01],
        [1.4044e-02, 5.9392e-02, 1.2804e-01, 1.2697e-03, 3.8775e-03, 7.3303e-01],
        [4.8016e-03, 1.3546e-02, 3.4415e-02, 4.4217e-04, 2.3949e-03, 2.3065e-01],
        [1.8502e-02, 8.4011e-02, 2.1282e-01, 3.0414e-03, 8.3536e-03, 9.4078e-01],
        [3.7046e-03, 4.9031e-01, 5.9418e-01, 2.9704e-03, 2.3252e-03, 9.7833e-01],
        [8.6604e-03, 5.4315e-02, 1.0441e-01, 7.5629e-04, 2.8981e-03, 6.5202e-01],
        [8.2812e-02, 7.9361e-01, 9.8650e-01, 2.1072e-02, 4.2896e-03, 9.9417e-01]])
[[0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]]
[[0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]]
                       precision    recall  f1-score   support

label_identity_attack       1.00      0.29      0.45       131
         label_insult       1.00      0.43      0.60       141
        label_obscene       1.00      0.78      0.87       170
label_severe_toxicity       1.00      0.24      0.39        25
         label_threat       1.00      0.23      0.37        40
       label_toxicity       1.00      0.74      0.85       158

            micro avg       1.00      0.55      0.71       665
            macro avg       1.00      0.45      0.59       665
         weighted avg       1.00      0.55      0.68       665
          samples avg       0.16      0.16      0.16       665

[[-2.0077057  -2.781364   -1.7500454  -5.7833786  -5.1459584   2.6019304 ]
 [-2.3641934  -3.1888514  -1.9253647  -5.715632   -4.3584876   2.5157344 ]
 [-6.620192   -5.6805673  -4.825888   -8.726649   -6.360818   -2.6941755 ]
 [-3.0865114  -2.147026   -1.4990041  -5.271564   -4.7958407   3.9392066 ]
 [ 4.958264    6.510385    6.4413867   0.9672206  -2.8837404   7.1610117 ]
 [-8.109295   -6.7684617  -5.8904324  -9.632868   -7.631772   -3.8588173 ]
 [-4.4942093  -3.638971   -2.1904821  -6.8492837  -5.35975     0.11336099]
 [-5.6642     -2.790094    3.0090714  -4.4393096  -5.451525    3.8125749 ]
 [-4.7597065  -3.5367484  -2.0940561  -6.953586   -5.74644    -0.04072533]
 [-7.1459813  -6.0040603  -5.2154245  -9.07795    -6.8007326  -3.1661634 ]]
tensor([[1.1840e-01, 5.8340e-02, 1.4804e-01, 3.0689e-03, 5.7892e-03, 9.3099e-01],
        [8.5944e-02, 3.9587e-02, 1.2726e-01, 3.2833e-03, 1.2636e-02, 9.2524e-01],
        [1.3314e-03, 3.4000e-03, 7.9556e-03, 1.6218e-04, 1.7250e-03, 6.3318e-02],
        [4.3667e-02, 1.0461e-01, 1.8257e-01, 5.1093e-03, 8.1963e-03, 9.8091e-01],
        [9.9302e-01, 9.9851e-01, 9.9841e-01, 7.2457e-01, 5.2963e-02, 9.9922e-01],
        [3.0064e-04, 1.1481e-03, 2.7582e-03, 6.5535e-05, 4.8457e-04, 2.0657e-02],
        [1.1050e-02, 2.5606e-02, 1.0061e-01, 1.0591e-03, 4.6801e-03, 5.2831e-01],
        [3.4559e-03, 5.7862e-02, 9.5298e-01, 1.1666e-02, 4.2714e-03, 9.7839e-01],
        [8.4953e-03, 2.8285e-02, 1.0968e-01, 9.5429e-04, 3.1840e-03, 4.8982e-01],
        [7.8740e-04, 2.4626e-03, 5.4028e-03, 1.1414e-04, 1.1117e-03, 4.0459e-02],
        [2.3153e-04, 7.6717e-04, 1.8308e-03, 6.4507e-05, 4.7652e-04, 1.2835e-02],
        [7.5299e-04, 2.2751e-03, 5.1159e-03, 1.1101e-04, 1.1013e-03, 3.8370e-02],
        [1.2145e-02, 1.0133e-01, 4.3918e-01, 4.7401e-03, 5.8265e-03, 9.6724e-01],
        [5.2549e-02, 9.9673e-01, 9.9422e-01, 5.4483e-02, 1.3416e-02, 9.9800e-01],
        [1.2601e-02, 1.2079e-01, 4.1746e-01, 5.2426e-03, 6.7586e-03, 9.7791e-01],
        [1.6833e-04, 4.7275e-04, 1.1108e-03, 7.4856e-05, 3.5607e-04, 7.4998e-03],
        [1.7595e-02, 5.4628e-02, 2.2218e-01, 2.6679e-03, 7.1091e-03, 8.8283e-01],
        [1.6769e-02, 5.5754e-02, 1.8045e-01, 3.0243e-03, 1.0789e-02, 9.4087e-01],
        [2.3110e-04, 7.6603e-04, 1.8333e-03, 6.4573e-05, 4.7791e-04, 1.2838e-02],
        [3.9925e-01, 6.5319e-01, 9.6449e-01, 1.8230e-02, 4.1518e-03, 9.9205e-01],
        [1.9103e-02, 2.8545e-02, 1.0896e-01, 1.3445e-03, 5.1196e-03, 6.1394e-01],
        [1.6990e-02, 6.3727e-02, 4.4615e-01, 8.4985e-03, 8.8691e-03, 9.8658e-01],
        [3.6052e-03, 7.8018e-03, 2.0645e-02, 3.5055e-04, 4.0958e-03, 1.8517e-01],
        [1.0782e-02, 2.4858e-02, 7.4568e-02, 8.6006e-04, 3.6202e-03, 4.7710e-01],
        [1.4044e-02, 5.9392e-02, 1.2804e-01, 1.2697e-03, 3.8775e-03, 7.3303e-01],
        [4.8016e-03, 1.3546e-02, 3.4415e-02, 4.4217e-04, 2.3949e-03, 2.3065e-01],
        [1.8502e-02, 8.4011e-02, 2.1282e-01, 3.0414e-03, 8.3536e-03, 9.4078e-01],
        [3.7046e-03, 4.9031e-01, 5.9418e-01, 2.9704e-03, 2.3252e-03, 9.7833e-01],
        [8.6604e-03, 5.4315e-02, 1.0441e-01, 7.5629e-04, 2.8981e-03, 6.5202e-01],
        [8.2812e-02, 7.9361e-01, 9.8650e-01, 2.1072e-02, 4.2896e-03, 9.9417e-01]])
[[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]
[[0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [1. 1. 1. 1. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1.]
 [0. 1. 1. 0. 0. 1.]]
END: ma 6.2.2023 11.10.26 +0200
