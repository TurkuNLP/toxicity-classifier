epochs: 4, learning rate: 2e-5, batch size: 8, prediction treshold: 0.5, model: xlm-roberta-base 
transfer from english train to translated finnish test
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 127656
    })
    dev: Dataset({
        features: ['text', 'labels'],
        num_rows: 31915
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.2669, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.0}
{'eval_loss': 0.170532688498497, 'eval_f1_micro': 0.648070230117916, 'eval_f1_weighted': 0.7243924846854025, 'eval_roc_auc': 0.9170781767330086, 'eval_accuracy': 0.891869027103243, 'eval_runtime': 257.3919, 'eval_samples_per_second': 123.994, 'eval_steps_per_second': 3.877, 'epoch': 1.0}
{'loss': 0.2168, 'learning_rate': 1e-05, 'epoch': 2.0}
{'eval_loss': 0.17669135332107544, 'eval_f1_micro': 0.7302544314482093, 'eval_f1_weighted': 0.7495277799662886, 'eval_roc_auc': 0.9111866958559475, 'eval_accuracy': 0.906940310198966, 'eval_runtime': 257.2119, 'eval_samples_per_second': 124.081, 'eval_steps_per_second': 3.88, 'epoch': 2.0}
{'loss': 0.1838, 'learning_rate': 5e-06, 'epoch': 3.0}
{'eval_loss': 0.16713756322860718, 'eval_f1_micro': 0.767929292929293, 'eval_f1_weighted': 0.7744539183570613, 'eval_roc_auc': 0.9150853411626855, 'eval_accuracy': 0.9146169512768291, 'eval_runtime': 257.0783, 'eval_samples_per_second': 124.145, 'eval_steps_per_second': 3.882, 'epoch': 3.0}
{'loss': 0.1471, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.17603197693824768, 'eval_f1_micro': 0.7791987472269346, 'eval_f1_weighted': 0.7842818421161603, 'eval_roc_auc': 0.9084768471159825, 'eval_accuracy': 0.9186902710324298, 'eval_runtime': 257.1078, 'eval_samples_per_second': 124.131, 'eval_steps_per_second': 3.882, 'epoch': 4.0}
{'train_runtime': 11605.6324, 'train_samples_per_second': 43.998, 'train_steps_per_second': 5.5, 'train_loss': 0.20365874110034976, 'epoch': 4.0}
{'epoch': 4.0,
 'eval_accuracy': 0.8660008127793929,
 'eval_f1_micro': 0.39344537815126057,
 'eval_f1_weighted': 0.36905064165886897,
 'eval_loss': 0.5963008999824524,
 'eval_roc_auc': 0.6552166559000981,
 'eval_runtime': 542.2315,
 'eval_samples_per_second': 117.99,
 'eval_steps_per_second': 3.688}
F1_micro: 0.39344537815126057
F1_weighted: 0.36905064165886897
END: ma 27.6.2022 18.48.51 +0300
