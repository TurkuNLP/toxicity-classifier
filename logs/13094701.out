epochs: 4, learning rate: 8e-6, batch size: 8, prediction treshold: 0.6, model: bert-base-cased 
original train and test data
Namespace(batch=8, binary=True, clean_as_label=False, dev=False, epochs=4, learning=8e-06, loss=True, model='bert-base-cased', test='data/test_en.jsonl', threshold=None, train=['data/train_en.jsonl'])
['data/train_en.jsonl']
tensor([18.9289,  3.3763,  3.1477, 16.6741, 55.6384,  1.7389], device='cuda:0')
DatasetDict({
    train: Dataset({
        features: ['text', 'labels'],
        num_rows: 159571
    })
    test: Dataset({
        features: ['text', 'labels'],
        num_rows: 63978
    })
})
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.1909, 'learning_rate': 6e-06, 'epoch': 1.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.17765037715435028, 'eval_accuracy': 0.9333208290349808, 'eval_roc_auc': 0.8944116449134384, 'eval_f1': 0.7123398516520566, 'eval_precision': 0.6151158728310236, 'eval_recall': 0.846067595707192, 'eval_runtime': 545.3883, 'eval_samples_per_second': 117.307, 'eval_steps_per_second': 3.667, 'epoch': 1.0}
{'loss': 0.1304, 'learning_rate': 4e-06, 'epoch': 2.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.15536360442638397, 'eval_accuracy': 0.9277720466410329, 'eval_roc_auc': 0.903194540733114, 'eval_f1': 0.7021975897402848, 'eval_precision': 0.5874487815397886, 'eval_recall': 0.8726573762614128, 'eval_runtime': 545.1915, 'eval_samples_per_second': 117.35, 'eval_steps_per_second': 3.668, 'epoch': 2.0}
{'loss': 0.097, 'learning_rate': 2e-06, 'epoch': 3.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.1902807652950287, 'eval_accuracy': 0.9118915877332833, 'eval_roc_auc': 0.9137530974155851, 'eval_f1': 0.6698682284040995, 'eval_precision': 0.5279726735598228, 'eval_recall': 0.9160659939131828, 'eval_runtime': 545.3923, 'eval_samples_per_second': 117.306, 'eval_steps_per_second': 3.667, 'epoch': 3.0}
{'loss': 0.0754, 'learning_rate': 0.0, 'epoch': 4.0}
Best threshold: 0.6499999999999999
{'eval_loss': 0.2141730785369873, 'eval_accuracy': 0.9123604989215043, 'eval_roc_auc': 0.9090842736330266, 'eval_f1': 0.6683622168332644, 'eval_precision': 0.5298199549887472, 'eval_recall': 0.905013615249079, 'eval_runtime': 545.0867, 'eval_samples_per_second': 117.372, 'eval_steps_per_second': 3.669, 'epoch': 4.0}
{'train_runtime': 14561.3229, 'train_samples_per_second': 43.834, 'train_steps_per_second': 5.479, 'train_loss': 0.12341562562666065, 'epoch': 4.0}
saved
Best threshold: 0.6499999999999999
F1: 0.7123398516520566
Best threshold: 0.6499999999999999
Best threshold: 0.3
              precision    recall  f1-score   support

       clean       0.99      0.91      0.95     57735
       toxic       0.53      0.91      0.67      6243

    accuracy                           0.91     63978
   macro avg       0.76      0.91      0.81     63978
weighted avg       0.94      0.91      0.92     63978

                       precision    recall  f1-score   support

label_identity_attack       0.44      0.72      0.55       712
         label_insult       0.45      0.91      0.60      3427
        label_obscene       0.49      0.92      0.64      3691
label_severe_toxicity       0.16      0.93      0.28       367
         label_threat       0.41      0.76      0.53       211
       label_toxicity       0.52      0.92      0.66      6090

            micro avg       0.46      0.90      0.61     14498
            macro avg       0.41      0.86      0.54     14498
         weighted avg       0.48      0.90      0.62     14498
          samples avg       0.07      0.09      0.07     14498

END: Mon Sep  5 00:04:50 EEST 2022
