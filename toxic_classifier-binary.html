<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module toxic_classifier-binary</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong>toxic_classifier-binary</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:/scratch/project_2005092/Anni/toxicity/toxic_classifier-binary.py">/scratch/project_2005092/Anni/toxicity/toxic_classifier-binary.py</a></font></td></tr></table>
    <p></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="argparse.html">argparse</a><br>
<a href="datasets.html">datasets</a><br>
</td><td width="25%" valign=top><a href="json.html">json</a><br>
<a href="logging.html">logging</a><br>
</td><td width="25%" valign=top><a href="numpy.html">numpy</a><br>
<a href="pandas.html">pandas</a><br>
</td><td width="25%" valign=top><a href="torch.html">torch</a><br>
<a href="transformers.html">transformers</a><br>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a>(<a href="builtins.html#object">builtins.object</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="toxic_classifier-binary.html#newTrainer">newTrainer</a>
</font></dt></dl>
</dd>
<dt><font face="helvetica, arial"><a href="transformers.trainer_callback.html#TrainerCallback">transformers.trainer_callback.TrainerCallback</a>(<a href="builtins.html#object">builtins.object</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="toxic_classifier-binary.html#LogSavingCallback">LogSavingCallback</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="LogSavingCallback">class <strong>LogSavingCallback</strong></a>(<a href="transformers.trainer_callback.html#TrainerCallback">transformers.trainer_callback.TrainerCallback</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>A&nbsp;class&nbsp;for&nbsp;objects&nbsp;that&nbsp;will&nbsp;inspect&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;training&nbsp;loop&nbsp;at&nbsp;some&nbsp;events&nbsp;and&nbsp;take&nbsp;some&nbsp;decisions.&nbsp;At<br>
each&nbsp;of&nbsp;those&nbsp;events&nbsp;the&nbsp;following&nbsp;arguments&nbsp;are&nbsp;available:<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;args&nbsp;([`TrainingArguments`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;training&nbsp;arguments&nbsp;used&nbsp;to&nbsp;instantiate&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`].<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;([`TrainerState`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;state&nbsp;of&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`].<br>
&nbsp;&nbsp;&nbsp;&nbsp;control&nbsp;([`TrainerControl`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;object&nbsp;that&nbsp;is&nbsp;returned&nbsp;to&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]&nbsp;and&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;make&nbsp;some&nbsp;decisions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;([`PreTrainedModel`]&nbsp;or&nbsp;`torch.nn.Module`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;model&nbsp;being&nbsp;trained.<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;([`PreTrainedTokenizer`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;tokenizer&nbsp;used&nbsp;for&nbsp;encoding&nbsp;the&nbsp;data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer&nbsp;(`torch.optim.Optimizer`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;optimizer&nbsp;used&nbsp;for&nbsp;the&nbsp;training&nbsp;steps.<br>
&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler&nbsp;(`torch.optim.lr_scheduler.LambdaLR`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;scheduler&nbsp;used&nbsp;for&nbsp;setting&nbsp;the&nbsp;learning&nbsp;rate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;train_dataloader&nbsp;(`torch.utils.data.DataLoader`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;dataloader&nbsp;used&nbsp;for&nbsp;training.<br>
&nbsp;&nbsp;&nbsp;&nbsp;eval_dataloader&nbsp;(`torch.utils.data.DataLoader`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;current&nbsp;dataloader&nbsp;used&nbsp;for&nbsp;training.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;metrics&nbsp;computed&nbsp;by&nbsp;the&nbsp;last&nbsp;evaluation&nbsp;phase.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Those&nbsp;are&nbsp;only&nbsp;accessible&nbsp;in&nbsp;the&nbsp;event&nbsp;`on_evaluate`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;logs&nbsp;&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;values&nbsp;to&nbsp;log.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Those&nbsp;are&nbsp;only&nbsp;accessible&nbsp;in&nbsp;the&nbsp;event&nbsp;`on_log`.<br>
&nbsp;<br>
The&nbsp;`control`&nbsp;object&nbsp;is&nbsp;the&nbsp;only&nbsp;one&nbsp;that&nbsp;can&nbsp;be&nbsp;changed&nbsp;by&nbsp;the&nbsp;callback,&nbsp;in&nbsp;which&nbsp;case&nbsp;the&nbsp;event&nbsp;that&nbsp;changes&nbsp;it<br>
should&nbsp;return&nbsp;the&nbsp;modified&nbsp;version.<br>
&nbsp;<br>
The&nbsp;argument&nbsp;`args`,&nbsp;`state`&nbsp;and&nbsp;`control`&nbsp;are&nbsp;positionals&nbsp;for&nbsp;all&nbsp;events,&nbsp;all&nbsp;the&nbsp;others&nbsp;are&nbsp;grouped&nbsp;in&nbsp;`kwargs`.<br>
You&nbsp;can&nbsp;unpack&nbsp;the&nbsp;ones&nbsp;you&nbsp;need&nbsp;in&nbsp;the&nbsp;signature&nbsp;of&nbsp;the&nbsp;event&nbsp;using&nbsp;them.&nbsp;As&nbsp;an&nbsp;example,&nbsp;see&nbsp;the&nbsp;code&nbsp;of&nbsp;the<br>
simple&nbsp;[`~transformer.PrinterCallback`].<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
```python<br>
class&nbsp;PrinterCallback(<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;<a href="#LogSavingCallback-on_log">on_log</a>(self,&nbsp;args,&nbsp;state,&nbsp;control,&nbsp;logs=None,&nbsp;**kwargs):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_&nbsp;=&nbsp;logs.pop("total_flos",&nbsp;None)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;state.is_local_process_zero:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(logs)<br>
```<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="toxic_classifier-binary.html#LogSavingCallback">LogSavingCallback</a></dd>
<dd><a href="transformers.trainer_callback.html#TrainerCallback">transformers.trainer_callback.TrainerCallback</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="LogSavingCallback-on_log"><strong>on_log</strong></a>(self, args, state, control, logs, model=None, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;after&nbsp;logging&nbsp;the&nbsp;last&nbsp;logs.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_train_begin"><strong>on_train_begin</strong></a>(self, *args, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;training.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_train_end"><strong>on_train_end</strong></a>(self, *args, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;training.</tt></dd></dl>

<hr>
Methods inherited from <a href="transformers.trainer_callback.html#TrainerCallback">transformers.trainer_callback.TrainerCallback</a>:<br>
<dl><dt><a name="LogSavingCallback-on_epoch_begin"><strong>on_epoch_begin</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;an&nbsp;epoch.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_epoch_end"><strong>on_epoch_end</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;an&nbsp;epoch.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_evaluate"><strong>on_evaluate</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;after&nbsp;an&nbsp;evaluation&nbsp;phase.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_init_end"><strong>on_init_end</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;the&nbsp;initialization&nbsp;of&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`].</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_prediction_step"><strong>on_prediction_step</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;after&nbsp;a&nbsp;prediction&nbsp;step.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_save"><strong>on_save</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;after&nbsp;a&nbsp;checkpoint&nbsp;save.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_step_begin"><strong>on_step_begin</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;a&nbsp;training&nbsp;step.&nbsp;If&nbsp;using&nbsp;gradient&nbsp;accumulation,&nbsp;one&nbsp;training&nbsp;step&nbsp;might&nbsp;take<br>
several&nbsp;inputs.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_step_end"><strong>on_step_end</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;a&nbsp;training&nbsp;step.&nbsp;If&nbsp;using&nbsp;gradient&nbsp;accumulation,&nbsp;one&nbsp;training&nbsp;step&nbsp;might&nbsp;take<br>
several&nbsp;inputs.</tt></dd></dl>

<dl><dt><a name="LogSavingCallback-on_substep_end"><strong>on_substep_end</strong></a>(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)</dt><dd><tt>Event&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;an&nbsp;substep&nbsp;during&nbsp;gradient&nbsp;accumulation.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="transformers.trainer_callback.html#TrainerCallback">transformers.trainer_callback.TrainerCallback</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table> <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="newTrainer">class <strong>newTrainer</strong></a>(<a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#newTrainer">newTrainer</a>(class_weights,&nbsp;**kwargs)<br>
&nbsp;<br>
A&nbsp;custom&nbsp;trainer&nbsp;to&nbsp;use&nbsp;a&nbsp;different&nbsp;loss&nbsp;and&nbsp;to&nbsp;use&nbsp;different&nbsp;class&nbsp;weights<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="toxic_classifier-binary.html#newTrainer">newTrainer</a></dd>
<dd><a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="newTrainer-__init__"><strong>__init__</strong></a>(self, class_weights, **kwargs)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="newTrainer-compute_loss"><strong>compute_loss</strong></a>(self, model, inputs, return_outputs=False)</dt><dd><tt>Computes&nbsp;loss&nbsp;with&nbsp;different&nbsp;class&nbsp;weights</tt></dd></dl>

<hr>
Methods inherited from <a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a>:<br>
<dl><dt><a name="newTrainer-add_callback"><strong>add_callback</strong></a>(self, callback)</dt><dd><tt>Add&nbsp;a&nbsp;callback&nbsp;to&nbsp;the&nbsp;current&nbsp;list&nbsp;of&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`].<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;callback&nbsp;(`type`&nbsp;or&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]&nbsp;class&nbsp;or&nbsp;an&nbsp;instance&nbsp;of&nbsp;a&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`].&nbsp;In&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;case,&nbsp;will&nbsp;instantiate&nbsp;a&nbsp;member&nbsp;of&nbsp;that&nbsp;class.</tt></dd></dl>

<dl><dt><a name="newTrainer-autocast_smart_context_manager"><strong>autocast_smart_context_manager</strong></a>(self)</dt><dd><tt>A&nbsp;helper&nbsp;wrapper&nbsp;that&nbsp;creates&nbsp;an&nbsp;appropriate&nbsp;context&nbsp;manager&nbsp;for&nbsp;`autocast`&nbsp;while&nbsp;feeding&nbsp;it&nbsp;the&nbsp;desired<br>
arguments,&nbsp;depending&nbsp;on&nbsp;the&nbsp;situation.</tt></dd></dl>

<dl><dt><a name="newTrainer-call_model_init"><strong>call_model_init</strong></a>(self, trial=None)</dt></dl>

<dl><dt><a name="newTrainer-create_model_card"><strong>create_model_card</strong></a>(self, language: Union[str, NoneType] = None, license: Union[str, NoneType] = None, tags: Union[str, NoneType] = None, model_name: Union[str, NoneType] = None, finetuned_from: Union[str, NoneType] = None, tasks: Union[str, NoneType] = None, dataset_tags: Union[str, List[str], NoneType] = None, dataset: Union[str, List[str], NoneType] = None, dataset_args: Union[str, List[str], NoneType] = None)</dt></dl>

<dl><dt><a name="newTrainer-create_optimizer"><strong>create_optimizer</strong></a>(self)</dt><dd><tt>Setup&nbsp;the&nbsp;optimizer.<br>
&nbsp;<br>
We&nbsp;provide&nbsp;a&nbsp;reasonable&nbsp;default&nbsp;that&nbsp;works&nbsp;well.&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;something&nbsp;else,&nbsp;you&nbsp;can&nbsp;pass&nbsp;a&nbsp;tuple&nbsp;in&nbsp;the<br>
<a href="transformers.trainer.html#Trainer">Trainer</a>'s&nbsp;init&nbsp;through&nbsp;`optimizers`,&nbsp;or&nbsp;subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;in&nbsp;a&nbsp;subclass.</tt></dd></dl>

<dl><dt><a name="newTrainer-create_optimizer_and_scheduler"><strong>create_optimizer_and_scheduler</strong></a>(self, num_training_steps: int)</dt><dd><tt>Setup&nbsp;the&nbsp;optimizer&nbsp;and&nbsp;the&nbsp;learning&nbsp;rate&nbsp;scheduler.<br>
&nbsp;<br>
We&nbsp;provide&nbsp;a&nbsp;reasonable&nbsp;default&nbsp;that&nbsp;works&nbsp;well.&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;something&nbsp;else,&nbsp;you&nbsp;can&nbsp;pass&nbsp;a&nbsp;tuple&nbsp;in&nbsp;the<br>
<a href="transformers.trainer.html#Trainer">Trainer</a>'s&nbsp;init&nbsp;through&nbsp;`optimizers`,&nbsp;or&nbsp;subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;(or&nbsp;`create_optimizer`&nbsp;and/or<br>
`create_scheduler`)&nbsp;in&nbsp;a&nbsp;subclass.</tt></dd></dl>

<dl><dt><a name="newTrainer-create_scheduler"><strong>create_scheduler</strong></a>(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)</dt><dd><tt>Setup&nbsp;the&nbsp;scheduler.&nbsp;The&nbsp;optimizer&nbsp;of&nbsp;the&nbsp;trainer&nbsp;must&nbsp;have&nbsp;been&nbsp;set&nbsp;up&nbsp;either&nbsp;before&nbsp;this&nbsp;method&nbsp;is&nbsp;called&nbsp;or<br>
passed&nbsp;as&nbsp;an&nbsp;argument.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;num_training_steps&nbsp;(int):&nbsp;The&nbsp;number&nbsp;of&nbsp;training&nbsp;steps&nbsp;to&nbsp;do.</tt></dd></dl>

<dl><dt><a name="newTrainer-evaluate"><strong>evaluate</strong></a>(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, NoneType] = None, ignore_keys: Union[List[str], NoneType] = None, metric_key_prefix: str = 'eval') -&gt; Dict[str, float]</dt><dd><tt>Run&nbsp;evaluation&nbsp;and&nbsp;returns&nbsp;metrics.<br>
&nbsp;<br>
The&nbsp;calling&nbsp;script&nbsp;will&nbsp;be&nbsp;responsible&nbsp;for&nbsp;providing&nbsp;a&nbsp;method&nbsp;to&nbsp;compute&nbsp;metrics,&nbsp;as&nbsp;they&nbsp;are&nbsp;task-dependent<br>
(pass&nbsp;it&nbsp;to&nbsp;the&nbsp;init&nbsp;`compute_metrics`&nbsp;argument).<br>
&nbsp;<br>
You&nbsp;can&nbsp;also&nbsp;subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;to&nbsp;inject&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;eval_dataset&nbsp;(`Dataset`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pass&nbsp;a&nbsp;dataset&nbsp;if&nbsp;you&nbsp;wish&nbsp;to&nbsp;override&nbsp;`self.<strong>eval_dataset</strong>`.&nbsp;If&nbsp;it&nbsp;is&nbsp;an&nbsp;`datasets.Dataset`,&nbsp;columns&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accepted&nbsp;by&nbsp;the&nbsp;`model.forward()`&nbsp;method&nbsp;are&nbsp;automatically&nbsp;removed.&nbsp;It&nbsp;must&nbsp;implement&nbsp;the&nbsp;`__len__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ignore_keys&nbsp;(`Lst[str]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;keys&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;your&nbsp;model&nbsp;(if&nbsp;it&nbsp;is&nbsp;a&nbsp;dictionary)&nbsp;that&nbsp;should&nbsp;be&nbsp;ignored&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gathering&nbsp;predictions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metric_key_prefix&nbsp;(`str`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`"eval"`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;optional&nbsp;prefix&nbsp;to&nbsp;be&nbsp;used&nbsp;as&nbsp;the&nbsp;metrics&nbsp;key&nbsp;prefix.&nbsp;For&nbsp;example&nbsp;the&nbsp;metrics&nbsp;"bleu"&nbsp;will&nbsp;be&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"eval_bleu"&nbsp;if&nbsp;the&nbsp;prefix&nbsp;is&nbsp;"eval"&nbsp;(default)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;dictionary&nbsp;containing&nbsp;the&nbsp;evaluation&nbsp;loss&nbsp;and&nbsp;the&nbsp;potential&nbsp;metrics&nbsp;computed&nbsp;from&nbsp;the&nbsp;predictions.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;dictionary&nbsp;also&nbsp;contains&nbsp;the&nbsp;epoch&nbsp;number&nbsp;which&nbsp;comes&nbsp;from&nbsp;the&nbsp;training&nbsp;state.</tt></dd></dl>

<dl><dt><a name="newTrainer-evaluation_loop"><strong>evaluation_loop</strong></a>(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Union[bool, NoneType] = None, ignore_keys: Union[List[str], NoneType] = None, metric_key_prefix: str = 'eval') -&gt; transformers.trainer_utils.EvalLoopOutput</dt><dd><tt>Prediction/evaluation&nbsp;loop,&nbsp;shared&nbsp;by&nbsp;`<a href="transformers.trainer.html#Trainer">Trainer</a>.<a href="#newTrainer-evaluate">evaluate</a>()`&nbsp;and&nbsp;`<a href="transformers.trainer.html#Trainer">Trainer</a>.<a href="#newTrainer-predict">predict</a>()`.<br>
&nbsp;<br>
Works&nbsp;both&nbsp;with&nbsp;or&nbsp;without&nbsp;labels.</tt></dd></dl>

<dl><dt><a name="newTrainer-floating_point_ops"><strong>floating_point_ops</strong></a>(self, inputs: Dict[str, Union[torch.Tensor, Any]])</dt><dd><tt>For&nbsp;models&nbsp;that&nbsp;inherit&nbsp;from&nbsp;[`PreTrainedModel`],&nbsp;uses&nbsp;that&nbsp;method&nbsp;to&nbsp;compute&nbsp;the&nbsp;number&nbsp;of&nbsp;floating&nbsp;point<br>
operations&nbsp;for&nbsp;every&nbsp;backward&nbsp;+&nbsp;forward&nbsp;pass.&nbsp;If&nbsp;using&nbsp;another&nbsp;model,&nbsp;either&nbsp;implement&nbsp;such&nbsp;a&nbsp;method&nbsp;in&nbsp;the<br>
model&nbsp;or&nbsp;subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs&nbsp;(`Dict[str,&nbsp;Union[torch.Tensor,&nbsp;Any]]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;inputs&nbsp;and&nbsp;targets&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;`int`:&nbsp;The&nbsp;number&nbsp;of&nbsp;floating-point&nbsp;operations.</tt></dd></dl>

<dl><dt><a name="newTrainer-get_eval_dataloader"><strong>get_eval_dataloader</strong></a>(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, NoneType] = None) -&gt; torch.utils.data.dataloader.DataLoader</dt><dd><tt>Returns&nbsp;the&nbsp;evaluation&nbsp;[`~torch.utils.data.DataLoader`].<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;inject&nbsp;some&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;eval_dataset&nbsp;(`torch.utils.data.Dataset`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;provided,&nbsp;will&nbsp;override&nbsp;`self.<strong>eval_dataset</strong>`.&nbsp;If&nbsp;it&nbsp;is&nbsp;an&nbsp;`datasets.Dataset`,&nbsp;columns&nbsp;not&nbsp;accepted&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;`model.forward()`&nbsp;method&nbsp;are&nbsp;automatically&nbsp;removed.&nbsp;It&nbsp;must&nbsp;implement&nbsp;`__len__`.</tt></dd></dl>

<dl><dt><a name="newTrainer-get_test_dataloader"><strong>get_test_dataloader</strong></a>(self, test_dataset: torch.utils.data.dataset.Dataset) -&gt; torch.utils.data.dataloader.DataLoader</dt><dd><tt>Returns&nbsp;the&nbsp;test&nbsp;[`~torch.utils.data.DataLoader`].<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;inject&nbsp;some&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;test_dataset&nbsp;(`torch.utils.data.Dataset`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;test&nbsp;dataset&nbsp;to&nbsp;use.&nbsp;If&nbsp;it&nbsp;is&nbsp;an&nbsp;`datasets.Dataset`,&nbsp;columns&nbsp;not&nbsp;accepted&nbsp;by&nbsp;the&nbsp;`model.forward()`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;method&nbsp;are&nbsp;automatically&nbsp;removed.&nbsp;It&nbsp;must&nbsp;implement&nbsp;`__len__`.</tt></dd></dl>

<dl><dt><a name="newTrainer-get_train_dataloader"><strong>get_train_dataloader</strong></a>(self) -&gt; torch.utils.data.dataloader.DataLoader</dt><dd><tt>Returns&nbsp;the&nbsp;training&nbsp;[`~torch.utils.data.DataLoader`].<br>
&nbsp;<br>
Will&nbsp;use&nbsp;no&nbsp;sampler&nbsp;if&nbsp;`train_dataset`&nbsp;does&nbsp;not&nbsp;implement&nbsp;`__len__`,&nbsp;a&nbsp;random&nbsp;sampler&nbsp;(adapted&nbsp;to&nbsp;distributed<br>
training&nbsp;if&nbsp;necessary)&nbsp;otherwise.<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;inject&nbsp;some&nbsp;custom&nbsp;behavior.</tt></dd></dl>

<dl><dt><a name="newTrainer-hyperparameter_search"><strong>hyperparameter_search</strong></a>(self, hp_space: Union[Callable[[ForwardRef('optuna.Trial')], Dict[str, float]], NoneType] = None, compute_objective: Union[Callable[[Dict[str, float]], float], NoneType] = None, n_trials: int = 20, direction: str = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Union[Callable[[ForwardRef('optuna.Trial')], str], NoneType] = None, **kwargs) -&gt; transformers.trainer_utils.BestRun</dt><dd><tt>Launch&nbsp;an&nbsp;hyperparameter&nbsp;search&nbsp;using&nbsp;`optuna`&nbsp;or&nbsp;`Ray&nbsp;Tune`&nbsp;or&nbsp;`SigOpt`.&nbsp;The&nbsp;optimized&nbsp;quantity&nbsp;is&nbsp;determined<br>
by&nbsp;`compute_objective`,&nbsp;which&nbsp;defaults&nbsp;to&nbsp;a&nbsp;function&nbsp;returning&nbsp;the&nbsp;evaluation&nbsp;loss&nbsp;when&nbsp;no&nbsp;metric&nbsp;is&nbsp;provided,<br>
the&nbsp;sum&nbsp;of&nbsp;all&nbsp;metrics&nbsp;otherwise.<br>
&nbsp;<br>
&lt;Tip&nbsp;warning={true}&gt;<br>
&nbsp;<br>
To&nbsp;use&nbsp;this&nbsp;method,&nbsp;you&nbsp;need&nbsp;to&nbsp;have&nbsp;provided&nbsp;a&nbsp;`model_init`&nbsp;when&nbsp;initializing&nbsp;your&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]:&nbsp;we&nbsp;need&nbsp;to<br>
reinitialize&nbsp;the&nbsp;model&nbsp;at&nbsp;each&nbsp;new&nbsp;run.&nbsp;This&nbsp;is&nbsp;incompatible&nbsp;with&nbsp;the&nbsp;`optimizers`&nbsp;argument,&nbsp;so&nbsp;you&nbsp;need&nbsp;to<br>
subclass&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]&nbsp;and&nbsp;override&nbsp;the&nbsp;method&nbsp;[`~<a href="transformers.trainer.html#Trainer">Trainer</a>.create_optimizer_and_scheduler`]&nbsp;for&nbsp;custom<br>
optimizer/scheduler.<br>
&nbsp;<br>
&lt;/Tip&gt;<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;hp_space&nbsp;(`Callable[["optuna.Trial"],&nbsp;Dict[str,&nbsp;float]]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;function&nbsp;that&nbsp;defines&nbsp;the&nbsp;hyperparameter&nbsp;search&nbsp;space.&nbsp;Will&nbsp;default&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`~trainer_utils.default_hp_space_optuna`]&nbsp;or&nbsp;[`~trainer_utils.default_hp_space_ray`]&nbsp;or<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[`~trainer_utils.default_hp_space_sigopt`]&nbsp;depending&nbsp;on&nbsp;your&nbsp;backend.<br>
&nbsp;&nbsp;&nbsp;&nbsp;compute_objective&nbsp;(`Callable[[Dict[str,&nbsp;float]],&nbsp;float]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;function&nbsp;computing&nbsp;the&nbsp;objective&nbsp;to&nbsp;minimize&nbsp;or&nbsp;maximize&nbsp;from&nbsp;the&nbsp;metrics&nbsp;returned&nbsp;by&nbsp;the&nbsp;`evaluate`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;method.&nbsp;Will&nbsp;default&nbsp;to&nbsp;[`~trainer_utils.default_compute_objective`].<br>
&nbsp;&nbsp;&nbsp;&nbsp;n_trials&nbsp;(`int`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;100):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;of&nbsp;trial&nbsp;runs&nbsp;to&nbsp;test.<br>
&nbsp;&nbsp;&nbsp;&nbsp;direction(`str`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`"minimize"`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Whether&nbsp;to&nbsp;optimize&nbsp;greater&nbsp;or&nbsp;lower&nbsp;objects.&nbsp;Can&nbsp;be&nbsp;`"minimize"`&nbsp;or&nbsp;`"maximize"`,&nbsp;you&nbsp;should&nbsp;pick<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`"minimize"`&nbsp;when&nbsp;optimizing&nbsp;the&nbsp;validation&nbsp;loss,&nbsp;`"maximize"`&nbsp;when&nbsp;optimizing&nbsp;one&nbsp;or&nbsp;several&nbsp;metrics.<br>
&nbsp;&nbsp;&nbsp;&nbsp;backend(`str`&nbsp;or&nbsp;[`~training_utils.HPSearchBackend`],&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;backend&nbsp;to&nbsp;use&nbsp;for&nbsp;hyperparameter&nbsp;search.&nbsp;Will&nbsp;default&nbsp;to&nbsp;optuna&nbsp;or&nbsp;Ray&nbsp;Tune&nbsp;or&nbsp;SigOpt,&nbsp;depending<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;which&nbsp;one&nbsp;is&nbsp;installed.&nbsp;If&nbsp;all&nbsp;are&nbsp;installed,&nbsp;will&nbsp;default&nbsp;to&nbsp;optuna.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;along&nbsp;to&nbsp;`optuna.create_study`&nbsp;or&nbsp;`ray.tune.run`.&nbsp;For&nbsp;more<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;information&nbsp;see:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;documentation&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;documentation&nbsp;of&nbsp;[tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;the&nbsp;documentation&nbsp;of&nbsp;[sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;[`trainer_utils.BestRun`]:&nbsp;All&nbsp;the&nbsp;information&nbsp;about&nbsp;the&nbsp;best&nbsp;run.</tt></dd></dl>

<dl><dt><a name="newTrainer-init_git_repo"><strong>init_git_repo</strong></a>(self, at_init: bool = False)</dt><dd><tt>Initializes&nbsp;a&nbsp;git&nbsp;repo&nbsp;in&nbsp;`self.<strong>args</strong>.hub_model_id`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;at_init&nbsp;(`bool`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`False`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Whether&nbsp;this&nbsp;function&nbsp;is&nbsp;called&nbsp;before&nbsp;any&nbsp;training&nbsp;or&nbsp;not.&nbsp;If&nbsp;`self.<strong>args</strong>.overwrite_output_dir`&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`True`&nbsp;and&nbsp;`at_init`&nbsp;is&nbsp;`True`,&nbsp;the&nbsp;path&nbsp;to&nbsp;the&nbsp;repo&nbsp;(which&nbsp;is&nbsp;`self.<strong>args</strong>.output_dir`)&nbsp;might&nbsp;be&nbsp;wiped<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;out.</tt></dd></dl>

<dl><dt><a name="newTrainer-is_local_process_zero"><strong>is_local_process_zero</strong></a>(self) -&gt; bool</dt><dd><tt>Whether&nbsp;or&nbsp;not&nbsp;this&nbsp;process&nbsp;is&nbsp;the&nbsp;local&nbsp;(e.g.,&nbsp;on&nbsp;one&nbsp;machine&nbsp;if&nbsp;training&nbsp;in&nbsp;a&nbsp;distributed&nbsp;fashion&nbsp;on&nbsp;several<br>
machines)&nbsp;main&nbsp;process.</tt></dd></dl>

<dl><dt><a name="newTrainer-is_world_process_zero"><strong>is_world_process_zero</strong></a>(self) -&gt; bool</dt><dd><tt>Whether&nbsp;or&nbsp;not&nbsp;this&nbsp;process&nbsp;is&nbsp;the&nbsp;global&nbsp;main&nbsp;process&nbsp;(when&nbsp;training&nbsp;in&nbsp;a&nbsp;distributed&nbsp;fashion&nbsp;on&nbsp;several<br>
machines,&nbsp;this&nbsp;is&nbsp;only&nbsp;going&nbsp;to&nbsp;be&nbsp;`True`&nbsp;for&nbsp;one&nbsp;process).</tt></dd></dl>

<dl><dt><a name="newTrainer-log"><strong>log</strong></a>(self, logs: Dict[str, float]) -&gt; None</dt><dd><tt>Log&nbsp;`logs`&nbsp;on&nbsp;the&nbsp;various&nbsp;objects&nbsp;watching&nbsp;training.<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;this&nbsp;method&nbsp;to&nbsp;inject&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;logs&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;values&nbsp;to&nbsp;log.</tt></dd></dl>

<dl><dt><a name="newTrainer-log_metrics"><strong>log_metrics</strong></a>(self, split, metrics)</dt><dd><tt>Log&nbsp;metrics&nbsp;in&nbsp;a&nbsp;specially&nbsp;formatted&nbsp;way<br>
&nbsp;<br>
Under&nbsp;distributed&nbsp;environment&nbsp;this&nbsp;is&nbsp;done&nbsp;only&nbsp;for&nbsp;a&nbsp;process&nbsp;with&nbsp;rank&nbsp;0.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;split&nbsp;(`str`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mode/split&nbsp;name:&nbsp;one&nbsp;of&nbsp;`train`,&nbsp;`eval`,&nbsp;`test`<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;metrics&nbsp;returned&nbsp;from&nbsp;train/evaluate/predictmetrics:&nbsp;metrics&nbsp;dict<br>
&nbsp;<br>
Notes&nbsp;on&nbsp;memory&nbsp;reports:<br>
&nbsp;<br>
In&nbsp;order&nbsp;to&nbsp;get&nbsp;memory&nbsp;usage&nbsp;report&nbsp;you&nbsp;need&nbsp;to&nbsp;install&nbsp;`psutil`.&nbsp;You&nbsp;can&nbsp;do&nbsp;that&nbsp;with&nbsp;`pip&nbsp;install&nbsp;psutil`.<br>
&nbsp;<br>
Now&nbsp;when&nbsp;this&nbsp;method&nbsp;is&nbsp;run,&nbsp;you&nbsp;will&nbsp;see&nbsp;a&nbsp;report&nbsp;that&nbsp;will&nbsp;include:&nbsp;:<br>
&nbsp;<br>
```<br>
init_mem_cpu_alloc_delta&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1301MB<br>
init_mem_cpu_peaked_delta&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;154MB<br>
init_mem_gpu_alloc_delta&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;230MB<br>
init_mem_gpu_peaked_delta&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0MB<br>
train_mem_cpu_alloc_delta&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1345MB<br>
train_mem_cpu_peaked_delta&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0MB<br>
train_mem_gpu_alloc_delta&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;693MB<br>
train_mem_gpu_peaked_delta&nbsp;=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7MB<br>
```<br>
&nbsp;<br>
**Understanding&nbsp;the&nbsp;reports:**<br>
&nbsp;<br>
-&nbsp;the&nbsp;first&nbsp;segment,&nbsp;e.g.,&nbsp;`train__`,&nbsp;tells&nbsp;you&nbsp;which&nbsp;stage&nbsp;the&nbsp;metrics&nbsp;are&nbsp;for.&nbsp;Reports&nbsp;starting&nbsp;with&nbsp;`init_`<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;be&nbsp;added&nbsp;to&nbsp;the&nbsp;first&nbsp;stage&nbsp;that&nbsp;gets&nbsp;run.&nbsp;So&nbsp;that&nbsp;if&nbsp;only&nbsp;evaluation&nbsp;is&nbsp;run,&nbsp;the&nbsp;memory&nbsp;usage&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;`__init__`&nbsp;will&nbsp;be&nbsp;reported&nbsp;along&nbsp;with&nbsp;the&nbsp;`eval_`&nbsp;metrics.<br>
-&nbsp;the&nbsp;third&nbsp;segment,&nbsp;is&nbsp;either&nbsp;`cpu`&nbsp;or&nbsp;`gpu`,&nbsp;tells&nbsp;you&nbsp;whether&nbsp;it's&nbsp;the&nbsp;general&nbsp;RAM&nbsp;or&nbsp;the&nbsp;gpu0&nbsp;memory<br>
&nbsp;&nbsp;&nbsp;&nbsp;metric.<br>
-&nbsp;`*_alloc_delta`&nbsp;-&nbsp;is&nbsp;the&nbsp;difference&nbsp;in&nbsp;the&nbsp;used/allocated&nbsp;memory&nbsp;counter&nbsp;between&nbsp;the&nbsp;end&nbsp;and&nbsp;the&nbsp;start&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;stage&nbsp;-&nbsp;it&nbsp;can&nbsp;be&nbsp;negative&nbsp;if&nbsp;a&nbsp;function&nbsp;released&nbsp;more&nbsp;memory&nbsp;than&nbsp;it&nbsp;allocated.<br>
-&nbsp;`*_peaked_delta`&nbsp;-&nbsp;is&nbsp;any&nbsp;extra&nbsp;memory&nbsp;that&nbsp;was&nbsp;consumed&nbsp;and&nbsp;then&nbsp;freed&nbsp;-&nbsp;relative&nbsp;to&nbsp;the&nbsp;current&nbsp;allocated<br>
&nbsp;&nbsp;&nbsp;&nbsp;memory&nbsp;counter&nbsp;-&nbsp;it&nbsp;is&nbsp;never&nbsp;negative.&nbsp;When&nbsp;you&nbsp;look&nbsp;at&nbsp;the&nbsp;metrics&nbsp;of&nbsp;any&nbsp;stage&nbsp;you&nbsp;add&nbsp;up&nbsp;`alloc_delta`&nbsp;+<br>
&nbsp;&nbsp;&nbsp;&nbsp;`peaked_delta`&nbsp;and&nbsp;you&nbsp;know&nbsp;how&nbsp;much&nbsp;memory&nbsp;was&nbsp;needed&nbsp;to&nbsp;complete&nbsp;that&nbsp;stage.<br>
&nbsp;<br>
The&nbsp;reporting&nbsp;happens&nbsp;only&nbsp;for&nbsp;process&nbsp;of&nbsp;rank&nbsp;0&nbsp;and&nbsp;gpu&nbsp;0&nbsp;(if&nbsp;there&nbsp;is&nbsp;a&nbsp;gpu).&nbsp;Typically&nbsp;this&nbsp;is&nbsp;enough&nbsp;since&nbsp;the<br>
main&nbsp;process&nbsp;does&nbsp;the&nbsp;bulk&nbsp;of&nbsp;work,&nbsp;but&nbsp;it&nbsp;could&nbsp;be&nbsp;not&nbsp;quite&nbsp;so&nbsp;if&nbsp;model&nbsp;parallel&nbsp;is&nbsp;used&nbsp;and&nbsp;then&nbsp;other&nbsp;GPUs&nbsp;may<br>
use&nbsp;a&nbsp;different&nbsp;amount&nbsp;of&nbsp;gpu&nbsp;memory.&nbsp;This&nbsp;is&nbsp;also&nbsp;not&nbsp;the&nbsp;same&nbsp;under&nbsp;DataParallel&nbsp;where&nbsp;gpu0&nbsp;may&nbsp;require&nbsp;much&nbsp;more<br>
memory&nbsp;than&nbsp;the&nbsp;rest&nbsp;since&nbsp;it&nbsp;stores&nbsp;the&nbsp;gradient&nbsp;and&nbsp;optimizer&nbsp;states&nbsp;for&nbsp;all&nbsp;participating&nbsp;GPUS.&nbsp;Perhaps&nbsp;in&nbsp;the<br>
future&nbsp;these&nbsp;reports&nbsp;will&nbsp;evolve&nbsp;to&nbsp;measure&nbsp;those&nbsp;too.<br>
&nbsp;<br>
The&nbsp;CPU&nbsp;RAM&nbsp;metric&nbsp;measures&nbsp;RSS&nbsp;(Resident&nbsp;Set&nbsp;Size)&nbsp;includes&nbsp;both&nbsp;the&nbsp;memory&nbsp;which&nbsp;is&nbsp;unique&nbsp;to&nbsp;the&nbsp;process&nbsp;and&nbsp;the<br>
memory&nbsp;shared&nbsp;with&nbsp;other&nbsp;processes.&nbsp;It&nbsp;is&nbsp;important&nbsp;to&nbsp;note&nbsp;that&nbsp;it&nbsp;does&nbsp;not&nbsp;include&nbsp;swapped&nbsp;out&nbsp;memory,&nbsp;so&nbsp;the<br>
reports&nbsp;could&nbsp;be&nbsp;imprecise.<br>
&nbsp;<br>
The&nbsp;CPU&nbsp;peak&nbsp;memory&nbsp;is&nbsp;measured&nbsp;using&nbsp;a&nbsp;sampling&nbsp;thread.&nbsp;Due&nbsp;to&nbsp;python's&nbsp;GIL&nbsp;it&nbsp;may&nbsp;miss&nbsp;some&nbsp;of&nbsp;the&nbsp;peak&nbsp;memory&nbsp;if<br>
that&nbsp;thread&nbsp;didn't&nbsp;get&nbsp;a&nbsp;chance&nbsp;to&nbsp;run&nbsp;when&nbsp;the&nbsp;highest&nbsp;memory&nbsp;was&nbsp;used.&nbsp;Therefore&nbsp;this&nbsp;report&nbsp;can&nbsp;be&nbsp;less&nbsp;than<br>
reality.&nbsp;Using&nbsp;`tracemalloc`&nbsp;would&nbsp;have&nbsp;reported&nbsp;the&nbsp;exact&nbsp;peak&nbsp;memory,&nbsp;but&nbsp;it&nbsp;doesn't&nbsp;report&nbsp;memory&nbsp;allocations<br>
outside&nbsp;of&nbsp;python.&nbsp;So&nbsp;if&nbsp;some&nbsp;C++&nbsp;CUDA&nbsp;extension&nbsp;allocated&nbsp;its&nbsp;own&nbsp;memory&nbsp;it&nbsp;won't&nbsp;be&nbsp;reported.&nbsp;And&nbsp;therefore&nbsp;it<br>
was&nbsp;dropped&nbsp;in&nbsp;favor&nbsp;of&nbsp;the&nbsp;memory&nbsp;sampling&nbsp;approach,&nbsp;which&nbsp;reads&nbsp;the&nbsp;current&nbsp;process&nbsp;memory&nbsp;usage.<br>
&nbsp;<br>
The&nbsp;GPU&nbsp;allocated&nbsp;and&nbsp;peak&nbsp;memory&nbsp;reporting&nbsp;is&nbsp;done&nbsp;with&nbsp;`torch.cuda.memory_allocated()`&nbsp;and<br>
`torch.cuda.max_memory_allocated()`.&nbsp;This&nbsp;metric&nbsp;reports&nbsp;only&nbsp;"deltas"&nbsp;for&nbsp;pytorch-specific&nbsp;allocations,&nbsp;as<br>
`torch.cuda`&nbsp;memory&nbsp;management&nbsp;system&nbsp;doesn't&nbsp;track&nbsp;any&nbsp;memory&nbsp;allocated&nbsp;outside&nbsp;of&nbsp;pytorch.&nbsp;For&nbsp;example,&nbsp;the&nbsp;very<br>
first&nbsp;cuda&nbsp;call&nbsp;typically&nbsp;loads&nbsp;CUDA&nbsp;kernels,&nbsp;which&nbsp;may&nbsp;take&nbsp;from&nbsp;0.5&nbsp;to&nbsp;2GB&nbsp;of&nbsp;GPU&nbsp;memory.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;this&nbsp;tracker&nbsp;doesn't&nbsp;account&nbsp;for&nbsp;memory&nbsp;allocations&nbsp;outside&nbsp;of&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]'s&nbsp;`__init__`,&nbsp;`train`,<br>
`evaluate`&nbsp;and&nbsp;`predict`&nbsp;calls.<br>
&nbsp;<br>
Because&nbsp;`evaluation`&nbsp;calls&nbsp;may&nbsp;happen&nbsp;during&nbsp;`train`,&nbsp;we&nbsp;can't&nbsp;handle&nbsp;nested&nbsp;invocations&nbsp;because<br>
`torch.cuda.max_memory_allocated`&nbsp;is&nbsp;a&nbsp;single&nbsp;counter,&nbsp;so&nbsp;if&nbsp;it&nbsp;gets&nbsp;reset&nbsp;by&nbsp;a&nbsp;nested&nbsp;eval&nbsp;call,&nbsp;`train`'s&nbsp;tracker<br>
will&nbsp;report&nbsp;incorrect&nbsp;info.&nbsp;If&nbsp;this&nbsp;[pytorch&nbsp;issue](https://github.com/pytorch/pytorch/issues/16266)&nbsp;gets&nbsp;resolved<br>
it&nbsp;will&nbsp;be&nbsp;possible&nbsp;to&nbsp;change&nbsp;this&nbsp;class&nbsp;to&nbsp;be&nbsp;re-entrant.&nbsp;Until&nbsp;then&nbsp;we&nbsp;will&nbsp;only&nbsp;track&nbsp;the&nbsp;outer&nbsp;level&nbsp;of<br>
`train`,&nbsp;`evaluate`&nbsp;and&nbsp;`predict`&nbsp;methods.&nbsp;Which&nbsp;means&nbsp;that&nbsp;if&nbsp;`eval`&nbsp;is&nbsp;called&nbsp;during&nbsp;`train`,&nbsp;it's&nbsp;the&nbsp;latter<br>
that&nbsp;will&nbsp;account&nbsp;for&nbsp;its&nbsp;memory&nbsp;usage&nbsp;and&nbsp;that&nbsp;of&nbsp;the&nbsp;former.<br>
&nbsp;<br>
This&nbsp;also&nbsp;means&nbsp;that&nbsp;if&nbsp;any&nbsp;other&nbsp;tool&nbsp;that&nbsp;is&nbsp;used&nbsp;along&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]&nbsp;calls<br>
`torch.cuda.reset_peak_memory_stats`,&nbsp;the&nbsp;gpu&nbsp;peak&nbsp;memory&nbsp;stats&nbsp;could&nbsp;be&nbsp;invalid.&nbsp;And&nbsp;the&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`]&nbsp;will&nbsp;disrupt<br>
the&nbsp;normal&nbsp;behavior&nbsp;of&nbsp;any&nbsp;such&nbsp;tools&nbsp;that&nbsp;rely&nbsp;on&nbsp;calling&nbsp;`torch.cuda.reset_peak_memory_stats`&nbsp;themselves.<br>
&nbsp;<br>
For&nbsp;best&nbsp;performance&nbsp;you&nbsp;may&nbsp;want&nbsp;to&nbsp;consider&nbsp;turning&nbsp;the&nbsp;memory&nbsp;profiling&nbsp;off&nbsp;for&nbsp;production&nbsp;runs.</tt></dd></dl>

<dl><dt><a name="newTrainer-metrics_format"><strong>metrics_format</strong></a>(self, metrics: Dict[str, float]) -&gt; Dict[str, float]</dt><dd><tt>Reformat&nbsp;<a href="transformers.trainer.html#Trainer">Trainer</a>&nbsp;metrics&nbsp;values&nbsp;to&nbsp;a&nbsp;human-readable&nbsp;format<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;metrics&nbsp;returned&nbsp;from&nbsp;train/evaluate/predict<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`):&nbsp;The&nbsp;reformatted&nbsp;metrics</tt></dd></dl>

<dl><dt><a name="newTrainer-num_examples"><strong>num_examples</strong></a>(self, dataloader: torch.utils.data.dataloader.DataLoader) -&gt; int</dt><dd><tt>Helper&nbsp;to&nbsp;get&nbsp;number&nbsp;of&nbsp;samples&nbsp;in&nbsp;a&nbsp;[`~torch.utils.data.DataLoader`]&nbsp;by&nbsp;accessing&nbsp;its&nbsp;dataset.&nbsp;When<br>
dataloader.dataset&nbsp;does&nbsp;not&nbsp;exist&nbsp;or&nbsp;has&nbsp;no&nbsp;length,&nbsp;estimates&nbsp;as&nbsp;best&nbsp;it&nbsp;can</tt></dd></dl>

<dl><dt><a name="newTrainer-pop_callback"><strong>pop_callback</strong></a>(self, callback)</dt><dd><tt>Remove&nbsp;a&nbsp;callback&nbsp;from&nbsp;the&nbsp;current&nbsp;list&nbsp;of&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]&nbsp;and&nbsp;returns&nbsp;it.<br>
&nbsp;<br>
If&nbsp;the&nbsp;callback&nbsp;is&nbsp;not&nbsp;found,&nbsp;returns&nbsp;`None`&nbsp;(and&nbsp;no&nbsp;error&nbsp;is&nbsp;raised).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;callback&nbsp;(`type`&nbsp;or&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]&nbsp;class&nbsp;or&nbsp;an&nbsp;instance&nbsp;of&nbsp;a&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`].&nbsp;In&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;case,&nbsp;will&nbsp;pop&nbsp;the&nbsp;first&nbsp;member&nbsp;of&nbsp;that&nbsp;class&nbsp;found&nbsp;in&nbsp;the&nbsp;list&nbsp;of&nbsp;callbacks.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]:&nbsp;The&nbsp;callback&nbsp;removed,&nbsp;if&nbsp;found.</tt></dd></dl>

<dl><dt><a name="newTrainer-predict"><strong>predict</strong></a>(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Union[List[str], NoneType] = None, metric_key_prefix: str = 'test') -&gt; transformers.trainer_utils.PredictionOutput</dt><dd><tt>Run&nbsp;prediction&nbsp;and&nbsp;returns&nbsp;predictions&nbsp;and&nbsp;potential&nbsp;metrics.<br>
&nbsp;<br>
Depending&nbsp;on&nbsp;the&nbsp;dataset&nbsp;and&nbsp;your&nbsp;use&nbsp;case,&nbsp;your&nbsp;test&nbsp;dataset&nbsp;may&nbsp;contain&nbsp;labels.&nbsp;In&nbsp;that&nbsp;case,&nbsp;this&nbsp;method<br>
will&nbsp;also&nbsp;return&nbsp;metrics,&nbsp;like&nbsp;in&nbsp;`<a href="#newTrainer-evaluate">evaluate</a>()`.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;test_dataset&nbsp;(`Dataset`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dataset&nbsp;to&nbsp;run&nbsp;the&nbsp;predictions&nbsp;on.&nbsp;If&nbsp;it&nbsp;is&nbsp;an&nbsp;`datasets.Dataset`,&nbsp;columns&nbsp;not&nbsp;accepted&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`model.forward()`&nbsp;method&nbsp;are&nbsp;automatically&nbsp;removed.&nbsp;Has&nbsp;to&nbsp;implement&nbsp;the&nbsp;method&nbsp;`__len__`<br>
&nbsp;&nbsp;&nbsp;&nbsp;ignore_keys&nbsp;(`Lst[str]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;keys&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;your&nbsp;model&nbsp;(if&nbsp;it&nbsp;is&nbsp;a&nbsp;dictionary)&nbsp;that&nbsp;should&nbsp;be&nbsp;ignored&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gathering&nbsp;predictions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;metric_key_prefix&nbsp;(`str`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`"test"`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;optional&nbsp;prefix&nbsp;to&nbsp;be&nbsp;used&nbsp;as&nbsp;the&nbsp;metrics&nbsp;key&nbsp;prefix.&nbsp;For&nbsp;example&nbsp;the&nbsp;metrics&nbsp;"bleu"&nbsp;will&nbsp;be&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"test_bleu"&nbsp;if&nbsp;the&nbsp;prefix&nbsp;is&nbsp;"test"&nbsp;(default)<br>
&nbsp;<br>
&lt;Tip&gt;<br>
&nbsp;<br>
If&nbsp;your&nbsp;predictions&nbsp;or&nbsp;labels&nbsp;have&nbsp;different&nbsp;sequence&nbsp;length&nbsp;(for&nbsp;instance&nbsp;because&nbsp;you're&nbsp;doing&nbsp;dynamic&nbsp;padding<br>
in&nbsp;a&nbsp;token&nbsp;classification&nbsp;task)&nbsp;the&nbsp;predictions&nbsp;will&nbsp;be&nbsp;padded&nbsp;(on&nbsp;the&nbsp;right)&nbsp;to&nbsp;allow&nbsp;for&nbsp;concatenation&nbsp;into<br>
one&nbsp;array.&nbsp;The&nbsp;padding&nbsp;index&nbsp;is&nbsp;-100.<br>
&nbsp;<br>
&lt;/Tip&gt;<br>
&nbsp;<br>
Returns:&nbsp;*NamedTuple*&nbsp;A&nbsp;namedtuple&nbsp;with&nbsp;the&nbsp;following&nbsp;keys:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;predictions&nbsp;(`np.ndarray`):&nbsp;The&nbsp;predictions&nbsp;on&nbsp;`test_dataset`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;label_ids&nbsp;(`np.ndarray`,&nbsp;*optional*):&nbsp;The&nbsp;labels&nbsp;(if&nbsp;the&nbsp;dataset&nbsp;contained&nbsp;some).<br>
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`,&nbsp;*optional*):&nbsp;The&nbsp;potential&nbsp;dictionary&nbsp;of&nbsp;metrics&nbsp;(if&nbsp;the&nbsp;dataset&nbsp;contained<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;labels).</tt></dd></dl>

<dl><dt><a name="newTrainer-prediction_loop"><strong>prediction_loop</strong></a>(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Union[bool, NoneType] = None, ignore_keys: Union[List[str], NoneType] = None, metric_key_prefix: str = 'eval') -&gt; transformers.trainer_utils.PredictionOutput</dt><dd><tt>Prediction/evaluation&nbsp;loop,&nbsp;shared&nbsp;by&nbsp;`<a href="transformers.trainer.html#Trainer">Trainer</a>.<a href="#newTrainer-evaluate">evaluate</a>()`&nbsp;and&nbsp;`<a href="transformers.trainer.html#Trainer">Trainer</a>.<a href="#newTrainer-predict">predict</a>()`.<br>
&nbsp;<br>
Works&nbsp;both&nbsp;with&nbsp;or&nbsp;without&nbsp;labels.</tt></dd></dl>

<dl><dt><a name="newTrainer-prediction_step"><strong>prediction_step</strong></a>(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Union[List[str], NoneType] = None) -&gt; Tuple[Union[torch.Tensor, NoneType], Union[torch.Tensor, NoneType], Union[torch.Tensor, NoneType]]</dt><dd><tt>Perform&nbsp;an&nbsp;evaluation&nbsp;step&nbsp;on&nbsp;`model`&nbsp;using&nbsp;`inputs`.<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;to&nbsp;inject&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;(`nn.Module`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;model&nbsp;to&nbsp;evaluate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs&nbsp;(`Dict[str,&nbsp;Union[torch.Tensor,&nbsp;Any]]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;inputs&nbsp;and&nbsp;targets&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dictionary&nbsp;will&nbsp;be&nbsp;unpacked&nbsp;before&nbsp;being&nbsp;fed&nbsp;to&nbsp;the&nbsp;model.&nbsp;Most&nbsp;models&nbsp;expect&nbsp;the&nbsp;targets&nbsp;under&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;argument&nbsp;`labels`.&nbsp;Check&nbsp;your&nbsp;model's&nbsp;documentation&nbsp;for&nbsp;all&nbsp;accepted&nbsp;arguments.<br>
&nbsp;&nbsp;&nbsp;&nbsp;prediction_loss_only&nbsp;(`bool`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Whether&nbsp;or&nbsp;not&nbsp;to&nbsp;return&nbsp;the&nbsp;loss&nbsp;only.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ignore_keys&nbsp;(`Lst[str]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;keys&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;your&nbsp;model&nbsp;(if&nbsp;it&nbsp;is&nbsp;a&nbsp;dictionary)&nbsp;that&nbsp;should&nbsp;be&nbsp;ignored&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gathering&nbsp;predictions.<br>
&nbsp;<br>
Return:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Tuple[Optional[torch.Tensor],&nbsp;Optional[torch.Tensor],&nbsp;Optional[torch.Tensor]]:&nbsp;A&nbsp;tuple&nbsp;with&nbsp;the&nbsp;loss,<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits&nbsp;and&nbsp;labels&nbsp;(each&nbsp;being&nbsp;optional).</tt></dd></dl>

<dl><dt><a name="newTrainer-push_to_hub"><strong>push_to_hub</strong></a>(self, commit_message: Union[str, NoneType] = 'End of training', blocking: bool = True, **kwargs) -&gt; str</dt><dd><tt>Upload&nbsp;*self.<strong>model</strong>*&nbsp;and&nbsp;*self.<strong>tokenizer</strong>*&nbsp;to&nbsp;the&nbsp;🤗&nbsp;model&nbsp;hub&nbsp;on&nbsp;the&nbsp;repo&nbsp;*self.<strong>args</strong>.hub_model_id*.<br>
&nbsp;<br>
Parameters:<br>
&nbsp;&nbsp;&nbsp;&nbsp;commit_message&nbsp;(`str`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`"End&nbsp;of&nbsp;training"`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Message&nbsp;to&nbsp;commit&nbsp;while&nbsp;pushing.<br>
&nbsp;&nbsp;&nbsp;&nbsp;blocking&nbsp;(`bool`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`True`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Whether&nbsp;the&nbsp;function&nbsp;should&nbsp;return&nbsp;only&nbsp;when&nbsp;the&nbsp;`git&nbsp;push`&nbsp;has&nbsp;finished.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;passed&nbsp;along&nbsp;to&nbsp;[`~<a href="transformers.trainer.html#Trainer">Trainer</a>.create_model_card`].<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;url&nbsp;of&nbsp;the&nbsp;commit&nbsp;of&nbsp;your&nbsp;model&nbsp;in&nbsp;the&nbsp;given&nbsp;repository&nbsp;if&nbsp;`blocking=False`,&nbsp;a&nbsp;tuple&nbsp;with&nbsp;the&nbsp;url&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;commit&nbsp;and&nbsp;an&nbsp;object&nbsp;to&nbsp;track&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;commit&nbsp;if&nbsp;`blocking=True`</tt></dd></dl>

<dl><dt><a name="newTrainer-remove_callback"><strong>remove_callback</strong></a>(self, callback)</dt><dd><tt>Remove&nbsp;a&nbsp;callback&nbsp;from&nbsp;the&nbsp;current&nbsp;list&nbsp;of&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`].<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;callback&nbsp;(`type`&nbsp;or&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`]&nbsp;class&nbsp;or&nbsp;an&nbsp;instance&nbsp;of&nbsp;a&nbsp;[`~transformer.<a href="transformers.trainer_callback.html#TrainerCallback">TrainerCallback</a>`].&nbsp;In&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;case,&nbsp;will&nbsp;remove&nbsp;the&nbsp;first&nbsp;member&nbsp;of&nbsp;that&nbsp;class&nbsp;found&nbsp;in&nbsp;the&nbsp;list&nbsp;of&nbsp;callbacks.</tt></dd></dl>

<dl><dt><a name="newTrainer-save_metrics"><strong>save_metrics</strong></a>(self, split, metrics, combined=True)</dt><dd><tt>Save&nbsp;metrics&nbsp;into&nbsp;a&nbsp;json&nbsp;file&nbsp;for&nbsp;that&nbsp;split,&nbsp;e.g.&nbsp;`train_results.json`.<br>
&nbsp;<br>
Under&nbsp;distributed&nbsp;environment&nbsp;this&nbsp;is&nbsp;done&nbsp;only&nbsp;for&nbsp;a&nbsp;process&nbsp;with&nbsp;rank&nbsp;0.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;split&nbsp;(`str`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mode/split&nbsp;name:&nbsp;one&nbsp;of&nbsp;`train`,&nbsp;`eval`,&nbsp;`test`,&nbsp;`all`<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics&nbsp;(`Dict[str,&nbsp;float]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;metrics&nbsp;returned&nbsp;from&nbsp;train/evaluate/predict<br>
&nbsp;&nbsp;&nbsp;&nbsp;combined&nbsp;(`bool`,&nbsp;*optional*,&nbsp;defaults&nbsp;to&nbsp;`True`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates&nbsp;combined&nbsp;metrics&nbsp;by&nbsp;updating&nbsp;`all_results.json`&nbsp;with&nbsp;metrics&nbsp;of&nbsp;this&nbsp;call<br>
&nbsp;<br>
To&nbsp;understand&nbsp;the&nbsp;metrics&nbsp;please&nbsp;read&nbsp;the&nbsp;docstring&nbsp;of&nbsp;[`~<a href="transformers.trainer.html#Trainer">Trainer</a>.log_metrics`].&nbsp;The&nbsp;only&nbsp;difference&nbsp;is&nbsp;that&nbsp;raw<br>
unformatted&nbsp;numbers&nbsp;are&nbsp;saved&nbsp;in&nbsp;the&nbsp;current&nbsp;method.</tt></dd></dl>

<dl><dt><a name="newTrainer-save_model"><strong>save_model</strong></a>(self, output_dir: Union[str, NoneType] = None, _internal_call: bool = False)</dt><dd><tt>Will&nbsp;save&nbsp;the&nbsp;model,&nbsp;so&nbsp;you&nbsp;can&nbsp;reload&nbsp;it&nbsp;using&nbsp;`from_pretrained()`.<br>
&nbsp;<br>
Will&nbsp;only&nbsp;save&nbsp;from&nbsp;the&nbsp;main&nbsp;process.</tt></dd></dl>

<dl><dt><a name="newTrainer-save_state"><strong>save_state</strong></a>(self)</dt><dd><tt>Saves&nbsp;the&nbsp;<a href="transformers.trainer.html#Trainer">Trainer</a>&nbsp;state,&nbsp;since&nbsp;<a href="transformers.trainer.html#Trainer">Trainer</a>.save_model&nbsp;saves&nbsp;only&nbsp;the&nbsp;tokenizer&nbsp;with&nbsp;the&nbsp;model<br>
&nbsp;<br>
Under&nbsp;distributed&nbsp;environment&nbsp;this&nbsp;is&nbsp;done&nbsp;only&nbsp;for&nbsp;a&nbsp;process&nbsp;with&nbsp;rank&nbsp;0.</tt></dd></dl>

<dl><dt><a name="newTrainer-store_flos"><strong>store_flos</strong></a>(self)</dt></dl>

<dl><dt><a name="newTrainer-train"><strong>train</strong></a>(self, resume_from_checkpoint: Union[str, bool, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), Dict[str, Any]] = None, ignore_keys_for_eval: Union[List[str], NoneType] = None, **kwargs)</dt><dd><tt>Main&nbsp;training&nbsp;entry&nbsp;point.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;resume_from_checkpoint&nbsp;(`str`&nbsp;or&nbsp;`bool`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;a&nbsp;`str`,&nbsp;local&nbsp;path&nbsp;to&nbsp;a&nbsp;saved&nbsp;checkpoint&nbsp;as&nbsp;saved&nbsp;by&nbsp;a&nbsp;previous&nbsp;instance&nbsp;of&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`].&nbsp;If&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`bool`&nbsp;and&nbsp;equals&nbsp;`True`,&nbsp;load&nbsp;the&nbsp;last&nbsp;checkpoint&nbsp;in&nbsp;*args.output_dir*&nbsp;as&nbsp;saved&nbsp;by&nbsp;a&nbsp;previous&nbsp;instance<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;[`<a href="transformers.trainer.html#Trainer">Trainer</a>`].&nbsp;If&nbsp;present,&nbsp;training&nbsp;will&nbsp;resume&nbsp;from&nbsp;the&nbsp;model/optimizer/scheduler&nbsp;states&nbsp;loaded&nbsp;here.<br>
&nbsp;&nbsp;&nbsp;&nbsp;trial&nbsp;(`optuna.Trial`&nbsp;or&nbsp;`Dict[str,&nbsp;Any]`,&nbsp;*optional*):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;trial&nbsp;run&nbsp;or&nbsp;the&nbsp;hyperparameter&nbsp;dictionary&nbsp;for&nbsp;hyperparameter&nbsp;search.<br>
&nbsp;&nbsp;&nbsp;&nbsp;ignore_keys_for_eval&nbsp;(`List[str]`,&nbsp;*optional*)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;keys&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;your&nbsp;model&nbsp;(if&nbsp;it&nbsp;is&nbsp;a&nbsp;dictionary)&nbsp;that&nbsp;should&nbsp;be&nbsp;ignored&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gathering&nbsp;predictions&nbsp;for&nbsp;evaluation&nbsp;during&nbsp;the&nbsp;training.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;used&nbsp;to&nbsp;hide&nbsp;deprecated&nbsp;arguments</tt></dd></dl>

<dl><dt><a name="newTrainer-training_step"><strong>training_step</strong></a>(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -&gt; torch.Tensor</dt><dd><tt>Perform&nbsp;a&nbsp;training&nbsp;step&nbsp;on&nbsp;a&nbsp;batch&nbsp;of&nbsp;inputs.<br>
&nbsp;<br>
Subclass&nbsp;and&nbsp;override&nbsp;to&nbsp;inject&nbsp;custom&nbsp;behavior.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;(`nn.Module`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;model&nbsp;to&nbsp;train.<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs&nbsp;(`Dict[str,&nbsp;Union[torch.Tensor,&nbsp;Any]]`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;inputs&nbsp;and&nbsp;targets&nbsp;of&nbsp;the&nbsp;model.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;dictionary&nbsp;will&nbsp;be&nbsp;unpacked&nbsp;before&nbsp;being&nbsp;fed&nbsp;to&nbsp;the&nbsp;model.&nbsp;Most&nbsp;models&nbsp;expect&nbsp;the&nbsp;targets&nbsp;under&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;argument&nbsp;`labels`.&nbsp;Check&nbsp;your&nbsp;model's&nbsp;documentation&nbsp;for&nbsp;all&nbsp;accepted&nbsp;arguments.<br>
&nbsp;<br>
Return:<br>
&nbsp;&nbsp;&nbsp;&nbsp;`torch.Tensor`:&nbsp;The&nbsp;tensor&nbsp;with&nbsp;training&nbsp;loss&nbsp;on&nbsp;this&nbsp;batch.</tt></dd></dl>

<hr>
Static methods inherited from <a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a>:<br>
<dl><dt><a name="newTrainer-get_optimizer_cls_and_kwargs"><strong>get_optimizer_cls_and_kwargs</strong></a>(args: transformers.training_args.TrainingArguments) -&gt; Tuple[Any, Any]</dt><dd><tt>Returns&nbsp;the&nbsp;optimizer&nbsp;class&nbsp;and&nbsp;optimizer&nbsp;parameters&nbsp;based&nbsp;on&nbsp;the&nbsp;training&nbsp;arguments.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;args&nbsp;(`transformers.training_args.TrainingArguments`):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;training&nbsp;arguments&nbsp;for&nbsp;the&nbsp;training&nbsp;session.</tt></dd></dl>

<hr>
Data descriptors inherited from <a href="transformers.trainer.html#Trainer">transformers.trainer.Trainer</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-compute_metrics"><strong>compute_metrics</strong></a>(pred)</dt><dd><tt>Computes&nbsp;the&nbsp;metrics</tt></dd></dl>
 <dl><dt><a name="-json_to_dataset"><strong>json_to_dataset</strong></a>(data)</dt><dd><tt>Reads&nbsp;the&nbsp;data&nbsp;from&nbsp;.jsonl&nbsp;format&nbsp;and&nbsp;turns&nbsp;it&nbsp;into&nbsp;a&nbsp;dataset&nbsp;using&nbsp;pandas.<br>
&nbsp;<br>
Parameters<br>
----------<br>
data:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;path&nbsp;to&nbsp;the&nbsp;file&nbsp;from&nbsp;which&nbsp;to&nbsp;get&nbsp;the&nbsp;data<br>
&nbsp;<br>
Returns<br>
-------<br>
dataset:&nbsp;Dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;data&nbsp;in&nbsp;dataset&nbsp;format</tt></dd></dl>
 <dl><dt><a name="-main"><strong>main</strong></a>()</dt></dl>
 <dl><dt><a name="-make_class_weights"><strong>make_class_weights</strong></a>(train)</dt><dd><tt>Calculates&nbsp;class&nbsp;weights&nbsp;for&nbsp;the&nbsp;loss&nbsp;function&nbsp;based&nbsp;on&nbsp;the&nbsp;train&nbsp;split.</tt></dd></dl>
 <dl><dt><a name="-predictions_to_csv"><strong>predictions_to_csv</strong></a>(trues, preds, dataset)</dt><dd><tt>Saves&nbsp;a&nbsp;dataframe&nbsp;to&nbsp;.csv&nbsp;with&nbsp;texts,&nbsp;correct&nbsp;labels&nbsp;and&nbsp;predicted&nbsp;labels&nbsp;to&nbsp;see&nbsp;what&nbsp;went&nbsp;right&nbsp;and&nbsp;what&nbsp;went&nbsp;wrong.<br>
&nbsp;<br>
Modified&nbsp;from&nbsp;https://gist.github.com/rap12391/ce872764fb927581e9d435e0decdc2df#file-output_df-ipynb<br>
&nbsp;<br>
Parameters<br>
---------<br>
trues:&nbsp;list<br>
&nbsp;&nbsp;&nbsp;&nbsp;list&nbsp;of&nbsp;correct&nbsp;labels<br>
preds:&nbsp;list<br>
&nbsp;&nbsp;&nbsp;&nbsp;list&nbsp;of&nbsp;predicted&nbsp;labels<br>
dataset:&nbsp;Dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;dataset&nbsp;from&nbsp;which&nbsp;to&nbsp;get&nbsp;texts</tt></dd></dl>
</td></tr></table>
</body></html>